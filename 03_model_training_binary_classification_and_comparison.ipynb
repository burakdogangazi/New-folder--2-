{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2c44e7",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73f6b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Scikit-Learn - Model Selection\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "\n",
    "# Scikit-Learn - Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Scikit-Learn - Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Visualization Settings\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Define paths for input and output\n",
    "FEATURE_PATH = os.path.join(\"data\", \"features\", \"combined_engineered_balanced_features.csv\")\n",
    "MODELS_DIR = os.path.join(\"data\", \"models\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6c950",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac840f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (70000, 30)\n",
      "Feature matrix shape: (70000, 25)\n",
      "\n",
      "Class distribution:\n",
      "label1\n",
      "Benign    35000\n",
      "Attack    35000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the feature-engineered dataset\n",
    "df = pd.read_csv(FEATURE_PATH)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Validate presence of required label column\n",
    "assert \"label1\" in df.columns, \"label1 column not found in dataset\"\n",
    "\n",
    "# Extract features and create binary target variable\n",
    "# Target: label1 (Attack=1, Benign=0)\n",
    "X = df.drop(columns=[\"label1\", \"label2\", \"label3\", \"label4\", \"label_full\"], errors='ignore')\n",
    "y = df[\"label1\"].apply(lambda x: 1 if str(x).lower() == \"attack\" else 0)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(y.value_counts().rename({0: \"Benign\", 1: \"Attack\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bacdaf6",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff967fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (56000, 25)\n",
      "Test set shape: (14000, 25)\n",
      "\n",
      "Training set class distribution:\n",
      "label1\n",
      "1    28000\n",
      "0    28000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Perform stratified train-test split to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"\\nTraining set class distribution:\\n{y_train.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc156dd",
   "metadata": {},
   "source": [
    "## 4. Metrics Computation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcf3df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, y_prob=None):\n",
    "    \"\"\"\n",
    "    Compute comprehensive binary classification metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    y_prob : array-like, optional\n",
    "        Predicted probabilities for positive class\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing computed metrics\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Extract confusion matrix components\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate specificity (true negative rate)\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    \n",
    "    # Calculate ROC-AUC if probabilities are provided\n",
    "    roc_auc = roc_auc_score(y_true, y_prob) if (y_prob is not None) else np.nan\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"specificity\": spec,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"confusion_matrix\": cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13bcb3b",
   "metadata": {},
   "source": [
    "## 5. Model Definition and Hyperparameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "372e3458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to train: ['log_reg', 'rf', 'lgb', 'mlp', 'knn', 'nb']\n",
      "Optimization method: RandomizedSearchCV\n",
      "Cross-validation folds: 3\n",
      "Iterations per model: 10\n",
      "Total models: 6\n"
     ]
    }
   ],
   "source": [
    "model_defs = {}\n",
    "\n",
    "# ============================================================================\n",
    "# LINEAR MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Logistic Regression - Fast baseline linear classifier\n",
    "model_defs[\"log_reg\"] = {\n",
    "    \"estimator\": LogisticRegression(max_iter=5000, random_state=RANDOM_STATE),\n",
    "    \"param_grid\": {\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"C\": [0.1, 1, 10, 100],\n",
    "        \"solver\": [\"lbfgs\", \"saga\"],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# TREE-BASED MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Random Forest - Ensemble of decision trees\n",
    "model_defs[\"rf\"] = {\n",
    "    \"estimator\": RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    \"param_grid\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [10, 20, 30, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\"],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# LightGBM - Fast gradient boosting classifier\n",
    "model_defs[\"lgb\"] = {\n",
    "    \"estimator\": lgb.LGBMClassifier(\n",
    "        random_state=RANDOM_STATE, \n",
    "        n_jobs=-1, \n",
    "        verbose=-1,\n",
    "        is_unbalance=True\n",
    "    ),\n",
    "    \"param_grid\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [5, 10, 15, -1],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"num_leaves\": [20, 30, 40, 50],\n",
    "        \"min_data_in_leaf\": [10, 20, 30],\n",
    "        \"feature_fraction\": [0.8, 0.9, 1.0],\n",
    "        \"bagging_fraction\": [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# NEURAL NETWORK MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Multi-Layer Perceptron - Neural network classifier\n",
    "model_defs[\"mlp\"] = {\n",
    "    \"estimator\": MLPClassifier(random_state=RANDOM_STATE, max_iter=500, early_stopping=True),\n",
    "    \"param_grid\": {\n",
    "        \"hidden_layer_sizes\": [(64,), (128,), (64, 32), (128, 64)],\n",
    "        \"activation\": [\"relu\", \"tanh\"],\n",
    "        \"alpha\": [0.0001, 0.001, 0.01],\n",
    "        \"learning_rate\": [\"constant\", \"adaptive\"],\n",
    "        \"batch_size\": [32, 64]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# DISTANCE-BASED & PROBABILISTIC MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# K-Nearest Neighbors - Distance-based instance classifier\n",
    "model_defs[\"knn\"] = {\n",
    "    \"estimator\": KNeighborsClassifier(),\n",
    "    \"param_grid\": {\n",
    "        \"n_neighbors\": [3, 5, 7, 9, 11],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"metric\": [\"euclidean\", \"manhattan\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Gaussian Naive Bayes - Probabilistic classifier\n",
    "model_defs[\"nb\"] = {\n",
    "    \"estimator\": GaussianNB(),\n",
    "    \"param_grid\": {\n",
    "        \"var_smoothing\": np.logspace(-10, -6, 5)\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODELS_TO_RUN = [\"log_reg\", \"rf\", \"lgb\", \"mlp\", \"knn\", \"nb\"]\n",
    "\n",
    "print(f\"Models to train: {MODELS_TO_RUN}\")\n",
    "print(f\"Optimization method: RandomizedSearchCV\")\n",
    "print(f\"Cross-validation folds: 3\")\n",
    "print(f\"Iterations per model: 10\")\n",
    "print(f\"Total models: {len(MODELS_TO_RUN)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4143fe",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8352d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_key, model_def, X_train, X_test, y_train, y_test,\n",
    "                             base_dir, cv_folds=3, n_iter=10):\n",
    "    \"\"\"\n",
    "    Train a model using RandomizedSearchCV and evaluate on test set.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_key : str\n",
    "        Model identifier\n",
    "    model_def : dict\n",
    "        Dictionary containing estimator and parameter grid\n",
    "    X_train, X_test : array-like\n",
    "        Training and test feature matrices\n",
    "    y_train, y_test : array-like\n",
    "        Training and test labels\n",
    "    base_dir : str\n",
    "        Base directory for saving results\n",
    "    cv_folds : int\n",
    "        Number of cross-validation folds\n",
    "    n_iter : int\n",
    "        Number of RandomizedSearchCV iterations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (results_dict, best_model, predictions, probabilities, metrics)\n",
    "    \"\"\"\n",
    "    estimator = model_def[\"estimator\"]\n",
    "    param_grid = model_def[\"param_grid\"]\n",
    "    \n",
    "    MODEL_DIR = os.path.join(base_dir, model_key)\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_key.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Perform hyperparameter tuning using RandomizedSearchCV\n",
    "    if param_grid:\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            scoring=\"f1\",\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            verbose=1\n",
    "        )\n",
    "        search.fit(X_train, y_train)\n",
    "        best_model = search.best_estimator_\n",
    "        best_params = search.best_params_\n",
    "        cv_f1 = search.best_score_\n",
    "    else:\n",
    "        best_model = estimator\n",
    "        best_model.fit(X_train, y_train)\n",
    "        best_params = {}\n",
    "        cv_f1 = np.nan\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    if not np.isnan(cv_f1):\n",
    "        print(f\"Cross-validation F1 score: {cv_f1:.4f}\")\n",
    "    else:\n",
    "        print(f\"Cross-validation F1 score: N/A\")\n",
    "    \n",
    "    # Generate predictions on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Extract probability estimates for ROC curve\n",
    "    if hasattr(best_model, \"predict_proba\"):\n",
    "        y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(best_model, \"decision_function\"):\n",
    "        y_prob = best_model.decision_function(X_test)\n",
    "        y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min() + 1e-10)\n",
    "    else:\n",
    "        y_prob = y_pred.astype(float)\n",
    "    \n",
    "    # Compute comprehensive metrics\n",
    "    metrics = compute_metrics(y_test, y_pred, y_prob)\n",
    "    \n",
    "    # Generate and save confusion matrix visualization\n",
    "    cm = metrics[\"confusion_matrix\"]\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Benign\", \"Attack\"], yticklabels=[\"Benign\", \"Attack\"],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(f\"{model_key.upper()} - Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    cm_path = os.path.join(MODEL_DIR, f\"{model_key}_confusion_matrix.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Generate and save ROC curve visualization\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2.5, label=f\"AUC = {roc_auc:.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw=1.5, label=\"Random Classifier\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=11)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=11)\n",
    "    plt.title(f\"ROC Curve - {model_key.upper()}\", fontsize=12)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    roc_path = os.path.join(MODEL_DIR, f\"{model_key}_roc_curve.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(roc_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Visualizations saved for {model_key}\")\n",
    "    \n",
    "    # Return summary metrics (include best_model for saving later)\n",
    "    return {\n",
    "        \"model\": model_key,\n",
    "        \"best_params\": str(best_params),\n",
    "        \"cv_f1_score\": cv_f1,\n",
    "        \"test_accuracy\": metrics[\"accuracy\"],\n",
    "        \"test_precision\": metrics[\"precision\"],\n",
    "        \"test_recall\": metrics[\"recall\"],\n",
    "        \"test_specificity\": metrics[\"specificity\"],\n",
    "        \"test_f1\": metrics[\"f1\"],\n",
    "        \"test_roc_auc\": metrics[\"roc_auc\"]\n",
    "    }, best_model, y_pred, y_prob, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc6dee",
   "metadata": {},
   "source": [
    "## 7. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebfbdba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: binary_classification\\results_20251210_174119\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training LOG_REG\n",
      "============================================================\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'class_weight': 'balanced', 'C': 100}\n",
      "Cross-validation F1 score: 0.8791\n",
      "Visualizations saved for log_reg\n",
      "\n",
      "============================================================\n",
      "Training RF\n",
      "============================================================\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'n_estimators': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': 'balanced'}\n",
      "Cross-validation F1 score: 0.9291\n",
      "Visualizations saved for rf\n",
      "\n",
      "============================================================\n",
      "Training LGB\n",
      "============================================================\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'num_leaves': 40, 'n_estimators': 300, 'min_data_in_leaf': 30, 'max_depth': -1, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 0.8}\n",
      "Cross-validation F1 score: 0.9343\n",
      "Visualizations saved for lgb\n",
      "\n",
      "============================================================\n",
      "Training MLP\n",
      "============================================================\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'learning_rate': 'adaptive', 'hidden_layer_sizes': (128, 64), 'batch_size': 64, 'alpha': 0.001, 'activation': 'tanh'}\n",
      "Cross-validation F1 score: 0.9410\n",
      "Visualizations saved for mlp\n",
      "\n",
      "============================================================\n",
      "Training KNN\n",
      "============================================================\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'weights': 'distance', 'n_neighbors': 5, 'metric': 'euclidean'}\n",
      "Cross-validation F1 score: 0.9226\n",
      "Visualizations saved for knn\n",
      "\n",
      "============================================================\n",
      "Training NB\n",
      "============================================================\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 5 is smaller than n_iter=10. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'var_smoothing': np.float64(1e-10)}\n",
      "Cross-validation F1 score: 0.7074\n",
      "Visualizations saved for nb\n",
      "\n",
      "============================================================\n",
      "MODEL TRAINING COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "Metrics summary saved: binary_classification\\results_20251210_174119\\01_metrics_summary_all_models.csv\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>cv_f1_score</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_specificity</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>{'solver': 'lbfgs', 'penalty': 'l2', 'class_we...</td>\n",
       "      <td>0.879062</td>\n",
       "      <td>0.886214</td>\n",
       "      <td>0.938240</td>\n",
       "      <td>0.826857</td>\n",
       "      <td>0.945571</td>\n",
       "      <td>0.879034</td>\n",
       "      <td>0.924381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf</td>\n",
       "      <td>{'n_estimators': 50, 'min_samples_split': 5, '...</td>\n",
       "      <td>0.929067</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.976824</td>\n",
       "      <td>0.891143</td>\n",
       "      <td>0.978857</td>\n",
       "      <td>0.932019</td>\n",
       "      <td>0.970790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lgb</td>\n",
       "      <td>{'num_leaves': 40, 'n_estimators': 300, 'min_d...</td>\n",
       "      <td>0.934332</td>\n",
       "      <td>0.940357</td>\n",
       "      <td>0.983681</td>\n",
       "      <td>0.895571</td>\n",
       "      <td>0.985143</td>\n",
       "      <td>0.937561</td>\n",
       "      <td>0.973518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mlp</td>\n",
       "      <td>{'learning_rate': 'adaptive', 'hidden_layer_si...</td>\n",
       "      <td>0.940961</td>\n",
       "      <td>0.944357</td>\n",
       "      <td>0.988535</td>\n",
       "      <td>0.899143</td>\n",
       "      <td>0.989571</td>\n",
       "      <td>0.941722</td>\n",
       "      <td>0.977413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'weights': 'distance', 'n_neighbors': 5, 'met...</td>\n",
       "      <td>0.922639</td>\n",
       "      <td>0.930143</td>\n",
       "      <td>0.974173</td>\n",
       "      <td>0.883714</td>\n",
       "      <td>0.976571</td>\n",
       "      <td>0.926742</td>\n",
       "      <td>0.958936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nb</td>\n",
       "      <td>{'var_smoothing': np.float64(1e-10)}</td>\n",
       "      <td>0.707357</td>\n",
       "      <td>0.752643</td>\n",
       "      <td>0.880077</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>0.920286</td>\n",
       "      <td>0.702823</td>\n",
       "      <td>0.829477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model                                        best_params  cv_f1_score  \\\n",
       "0  log_reg  {'solver': 'lbfgs', 'penalty': 'l2', 'class_we...     0.879062   \n",
       "1       rf  {'n_estimators': 50, 'min_samples_split': 5, '...     0.929067   \n",
       "2      lgb  {'num_leaves': 40, 'n_estimators': 300, 'min_d...     0.934332   \n",
       "3      mlp  {'learning_rate': 'adaptive', 'hidden_layer_si...     0.940961   \n",
       "4      knn  {'weights': 'distance', 'n_neighbors': 5, 'met...     0.922639   \n",
       "5       nb               {'var_smoothing': np.float64(1e-10)}     0.707357   \n",
       "\n",
       "   test_accuracy  test_precision  test_recall  test_specificity   test_f1  \\\n",
       "0       0.886214        0.938240     0.826857          0.945571  0.879034   \n",
       "1       0.935000        0.976824     0.891143          0.978857  0.932019   \n",
       "2       0.940357        0.983681     0.895571          0.985143  0.937561   \n",
       "3       0.944357        0.988535     0.899143          0.989571  0.941722   \n",
       "4       0.930143        0.974173     0.883714          0.976571  0.926742   \n",
       "5       0.752643        0.880077     0.585000          0.920286  0.702823   \n",
       "\n",
       "   test_roc_auc  \n",
       "0      0.924381  \n",
       "1      0.970790  \n",
       "2      0.973518  \n",
       "3      0.977413  \n",
       "4      0.958936  \n",
       "5      0.829477  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BEST MODEL: MLP\n",
      "Test F1-Score: 0.9417\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate timestamp for unique results directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = os.path.join(\"binary_classification\", f\"results_{timestamp}\")\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved to: {RUN_DIR}\\n\")\n",
    "\n",
    "results_list = []\n",
    "models_dict = {}\n",
    "y_preds_dict = {}\n",
    "y_probs_dict = {}\n",
    "metrics_dict = {}\n",
    "\n",
    "# Train all models using optimized RandomizedSearchCV\n",
    "for key in MODELS_TO_RUN:\n",
    "    result_row, best_model, y_pred, y_prob, metrics = train_and_evaluate_model(\n",
    "        model_key=key,\n",
    "        model_def=model_defs[key],\n",
    "        X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test,\n",
    "        base_dir=RUN_DIR,\n",
    "        cv_folds=3,\n",
    "        n_iter=10\n",
    "    )\n",
    "    results_list.append(result_row)\n",
    "    models_dict[key] = best_model\n",
    "    y_preds_dict[key] = y_pred\n",
    "    y_probs_dict[key] = y_prob\n",
    "    metrics_dict[key] = metrics\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL TRAINING COMPLETED SUCCESSFULLY\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Create metrics summary dataframe\n",
    "metrics_df = pd.DataFrame(results_list)\n",
    "metrics_csv = os.path.join(RUN_DIR, \"01_metrics_summary_all_models.csv\")\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "print(f\"Metrics summary saved: {metrics_csv}\\n\")\n",
    "display(metrics_df)\n",
    "\n",
    "# Identify best performing model\n",
    "best_model_key = metrics_df.loc[metrics_df[\"test_f1\"].idxmax(), \"model\"]\n",
    "best_model_f1 = metrics_df.loc[metrics_df[\"test_f1\"].idxmax(), \"test_f1\"]\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL: {best_model_key.upper()}\")\n",
    "print(f\"Test F1-Score: {best_model_f1:.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9590c420",
   "metadata": {},
   "source": [
    "## 8. Best Model Detailed Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52aa2506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved: binary_classification\\results_20251210_174119\\best_binary_model_mlp.pkl\n",
      "\n",
      "Best model also saved to production: data\\models\\best_binary_classification_model.pkl\n",
      "\n",
      "Best model report generated and saved\n",
      "\n",
      "======================================================================\n",
      "BINARY INTRUSION DETECTION - BEST MODEL REPORT\n",
      "======================================================================\n",
      "\n",
      "Execution Timestamp: 20251210_174119\n",
      "Best Model: MLP\n",
      "Results Location: binary_classification\\results_20251210_174119\\best_binary_model_mlp.pkl\n",
      "Production Location: data\\models\\best_binary_classification_model.pkl\n",
      "\n",
      "======================================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "======================================================================\n",
      "\n",
      "Accuracy:       0.9444\n",
      "Precision:      0.9885\n",
      "Recall:         0.8991\n",
      "Specificity:    0.9896\n",
      "F1-Score:       0.9417\n",
      "ROC-AUC:        0.9774\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6927   73]\n",
      " [ 706 6294]]\n",
      "\n",
      "======================================================================\n",
      "CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign     0.9075    0.9896    0.9468      7000\n",
      "      Attack     0.9885    0.8991    0.9417      7000\n",
      "\n",
      "    accuracy                         0.9444     14000\n",
      "   macro avg     0.9480    0.9444    0.9442     14000\n",
      "weighted avg     0.9480    0.9444    0.9442     14000\n",
      "\n",
      "\n",
      "======================================================================\n",
      "BEST MODEL HYPERPARAMETERS\n",
      "======================================================================\n",
      "\n",
      "{'hidden_layer_sizes': [(64,), (128,), (64, 32), (128, 64)], 'activation': ['relu', 'tanh'], 'alpha': [0.0001, 0.001, 0.01], 'learning_rate': ['constant', 'adaptive'], 'batch_size': [32, 64]}\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract best model results\n",
    "best_model_obj = models_dict[best_model_key]\n",
    "y_pred_best = y_preds_dict[best_model_key]\n",
    "y_prob_best = y_probs_dict[best_model_key]\n",
    "metrics_best = metrics_dict[best_model_key]\n",
    "\n",
    "# Save best model to its results directory\n",
    "best_model_path = os.path.join(RUN_DIR, f\"best_binary_model_{best_model_key}.pkl\")\n",
    "joblib.dump(best_model_obj, best_model_path)\n",
    "print(f\"Best model saved: {best_model_path}\\n\")\n",
    "\n",
    "# Also save to production directory for deployment\n",
    "best_model_prod_path = os.path.join(MODELS_DIR, f\"best_binary_classification_model.pkl\")\n",
    "joblib.dump(best_model_obj, best_model_prod_path)\n",
    "print(f\"Best model also saved to production: {best_model_prod_path}\\n\")\n",
    "\n",
    "# Generate classification report for best model\n",
    "class_report = classification_report(y_test, y_pred_best,\n",
    "                                     target_names=[\"Benign\", \"Attack\"],\n",
    "                                     digits=4)\n",
    "\n",
    "# Create detailed report text\n",
    "report_text = f\"\"\"\n",
    "{'='*70}\n",
    "BINARY INTRUSION DETECTION - BEST MODEL REPORT\n",
    "{'='*70}\n",
    "\n",
    "Execution Timestamp: {timestamp}\n",
    "Best Model: {best_model_key.upper()}\n",
    "Results Location: {best_model_path}\n",
    "Production Location: {best_model_prod_path}\n",
    "\n",
    "{'='*70}\n",
    "MODEL PERFORMANCE METRICS\n",
    "{'='*70}\n",
    "\n",
    "Accuracy:       {metrics_best['accuracy']:.4f}\n",
    "Precision:      {metrics_best['precision']:.4f}\n",
    "Recall:         {metrics_best['recall']:.4f}\n",
    "Specificity:    {metrics_best['specificity']:.4f}\n",
    "F1-Score:       {metrics_best['f1']:.4f}\n",
    "ROC-AUC:        {metrics_best['roc_auc']:.4f}\n",
    "\n",
    "Confusion Matrix:\n",
    "{metrics_best['confusion_matrix']}\n",
    "\n",
    "{'='*70}\n",
    "CLASSIFICATION REPORT\n",
    "{'='*70}\n",
    "\n",
    "{class_report}\n",
    "\n",
    "{'='*70}\n",
    "BEST MODEL HYPERPARAMETERS\n",
    "{'='*70}\n",
    "\n",
    "{str(model_defs[best_model_key]['param_grid'])}\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "# Save report to file\n",
    "report_path = os.path.join(RUN_DIR, \"02_best_model_report.txt\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(\"Best model report generated and saved\")\n",
    "print(report_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ddbec",
   "metadata": {},
   "source": [
    "## 9. Model Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6d1f43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: models_comparison_metrics.png\n",
      "Saved: roc_curves_comparison.png\n",
      "Saved: f1_score_ranking.png\n",
      "Saved: metrics_heatmap.png\n",
      "\n",
      "All comparison visualizations generated successfully\n"
     ]
    }
   ],
   "source": [
    "# 1. Performance metrics comparison across models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle(\"Model Performance Comparison\", fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = [\"test_accuracy\", \"test_precision\", \"test_recall\",\n",
    "                   \"test_specificity\", \"test_f1\", \"test_roc_auc\"]\n",
    "colors = ['#FF6B6B' if model == best_model_key else '#4ECDC4'\n",
    "          for model in metrics_df[\"model\"]]\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    bars = ax.bar(metrics_df[\"model\"], metrics_df[metric], color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax.set_ylabel(metric.replace(\"test_\", \"\"), fontsize=10)\n",
    "    ax.set_title(metric.replace(\"test_\", \"\").upper(), fontsize=11, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_path = os.path.join(RUN_DIR, \"03_models_comparison_metrics.png\")\n",
    "plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: models_comparison_metrics.png\")\n",
    "\n",
    "# 2. ROC curves comparison\n",
    "plt.figure(figsize=(10, 8))\n",
    "for model_key in MODELS_TO_RUN:\n",
    "    y_prob = y_probs_dict[model_key]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    line_width = 3 if model_key == best_model_key else 1.5\n",
    "    line_style = '-' if model_key == best_model_key else '--'\n",
    "    alpha = 1.0 if model_key == best_model_key else 0.7\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=line_width, linestyle=line_style, alpha=alpha,\n",
    "             label=f\"{model_key.upper()} (AUC={auc:.4f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "plt.title(\"ROC Curves - All Models Comparison\", fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "roc_comparison_path = os.path.join(RUN_DIR, \"04_roc_curves_comparison.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(roc_comparison_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: roc_curves_comparison.png\")\n",
    "\n",
    "# 3. F1-score ranking\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sorted_df = metrics_df.sort_values(\"test_f1\", ascending=True)\n",
    "colors_rank = ['#FF6B6B' if model == best_model_key else '#95E1D3'\n",
    "               for model in sorted_df[\"model\"]]\n",
    "bars = ax.barh(sorted_df[\"model\"], sorted_df[\"test_f1\"], color=colors_rank, edgecolor='black', alpha=0.85)\n",
    "\n",
    "ax.set_xlabel(\"F1-Score\", fontsize=12)\n",
    "ax.set_title(\"Models Ranked by F1-Score\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([0, 1.05])\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for idx, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "            f' {width:.4f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "ranking_path = os.path.join(RUN_DIR, \"05_f1_score_ranking.png\")\n",
    "plt.savefig(ranking_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: f1_score_ranking.png\")\n",
    "\n",
    "# 4. Metrics heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "metrics_for_heatmap = metrics_df[[\"model\", \"test_accuracy\", \"test_precision\",\n",
    "                                   \"test_recall\", \"test_specificity\", \"test_f1\", \"test_roc_auc\"]].set_index(\"model\")\n",
    "sns.heatmap(metrics_for_heatmap.T, annot=True, fmt='.4f', cmap='RdYlGn',\n",
    "            cbar_kws={'label': 'Score'}, ax=ax, linewidths=0.5)\n",
    "ax.set_title(\"Model Metrics Heatmap\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "heatmap_path = os.path.join(RUN_DIR, \"06_metrics_heatmap.png\")\n",
    "plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: metrics_heatmap.png\")\n",
    "\n",
    "print(\"\\nAll comparison visualizations generated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b912cdc",
   "metadata": {},
   "source": [
    "## 10. Execution Summary and Results Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0333236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESULTS DIRECTORY STRUCTURE\n",
      "======================================================================\n",
      "\n",
      "results_20251210_174119/\n",
      "  01_metrics_summary_all_models.csv (1.5KB)\n",
      "  02_best_model_report.txt (1.7KB)\n",
      "  03_models_comparison_metrics.png (307.1KB)\n",
      "  04_roc_curves_comparison.png (317.3KB)\n",
      "  05_f1_score_ranking.png (86.1KB)\n",
      "  06_metrics_heatmap.png (295.0KB)\n",
      "  best_binary_model_mlp.pkl (282.1KB)\n",
      "  knn/\n",
      "    knn_confusion_matrix.png (74.5KB)\n",
      "    knn_roc_curve.png (107.9KB)\n",
      "  lgb/\n",
      "    lgb_confusion_matrix.png (75.1KB)\n",
      "    lgb_roc_curve.png (105.8KB)\n",
      "  log_reg/\n",
      "    log_reg_confusion_matrix.png (76.9KB)\n",
      "    log_reg_roc_curve.png (110.8KB)\n",
      "  mlp/\n",
      "    mlp_confusion_matrix.png (73.7KB)\n",
      "    mlp_roc_curve.png (103.9KB)\n",
      "  nb/\n",
      "    nb_confusion_matrix.png (75.5KB)\n",
      "    nb_roc_curve.png (115.4KB)\n",
      "  rf/\n",
      "    rf_confusion_matrix.png (74.8KB)\n",
      "    rf_roc_curve.png (105.1KB)\n",
      "\n",
      "======================================================================\n",
      "EXECUTION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "\n",
      "BINARY CLASSIFICATION MODEL TRAINING SUMMARY\n",
      "\n",
      "Total Models Trained: 6\n",
      "Models: LOG_REG, RF, LGB, MLP, KNN, NB\n",
      "\n",
      "Best Performing Model: MLP\n",
      "Best F1-Score: 0.9417\n",
      "\n",
      "Results Location: c:\\Users\\burak.dogan.2\\Desktop\\Projeler\\SC\\AI-IDS\\binary_classification\\results_20251210_174119\n",
      "\n",
      "Generated Files:\n",
      "- 01_metrics_summary_all_models.csv: Comprehensive metrics for all models\n",
      "- 02_best_model_report.txt: Detailed analysis of the best model\n",
      "- 03_models_comparison_metrics.png: Performance metrics comparison\n",
      "- 04_roc_curves_comparison.png: ROC curves for all models\n",
      "- 05_f1_score_ranking.png: Model ranking by F1-score\n",
      "- 06_metrics_heatmap.png: Metrics heatmap visualization\n",
      "\n",
      "Model-Specific Outputs:\n",
      "\n",
      "  LOG_REG:\n",
      "    - log_reg_confusion_matrix.png\n",
      "    - log_reg_roc_curve.png\n",
      "\n",
      "  RF:\n",
      "    - rf_confusion_matrix.png\n",
      "    - rf_roc_curve.png\n",
      "\n",
      "  LGB:\n",
      "    - lgb_confusion_matrix.png\n",
      "    - lgb_roc_curve.png\n",
      "\n",
      "  MLP:\n",
      "    - mlp_confusion_matrix.png\n",
      "    - mlp_roc_curve.png\n",
      "\n",
      "  KNN:\n",
      "    - knn_confusion_matrix.png\n",
      "    - knn_roc_curve.png\n",
      "\n",
      "  NB:\n",
      "    - nb_confusion_matrix.png\n",
      "    - nb_roc_curve.png\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Summary saved to: binary_classification\\results_20251210_174119\\00_EXECUTION_SUMMARY.txt\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RESULTS DIRECTORY STRUCTURE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# List all generated files in results directory\n",
    "for root, dirs, files in os.walk(RUN_DIR):\n",
    "    level = root.replace(RUN_DIR, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    sub_indent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        size_str = f\"{file_size / 1024:.1f}KB\" if file_size > 1024 else f\"{file_size}B\"\n",
    "        print(f'{sub_indent}{file} ({size_str})')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Generate execution summary\n",
    "summary_text = f\"\"\"\n",
    "BINARY CLASSIFICATION MODEL TRAINING SUMMARY\n",
    "\n",
    "Total Models Trained: {len(MODELS_TO_RUN)}\n",
    "Models: {', '.join([m.upper() for m in MODELS_TO_RUN])}\n",
    "\n",
    "Best Performing Model: {best_model_key.upper()}\n",
    "Best F1-Score: {best_model_f1:.4f}\n",
    "\n",
    "Results Location: {os.path.abspath(RUN_DIR)}\n",
    "\n",
    "Generated Files:\n",
    "- 01_metrics_summary_all_models.csv: Comprehensive metrics for all models\n",
    "- 02_best_model_report.txt: Detailed analysis of the best model\n",
    "- 03_models_comparison_metrics.png: Performance metrics comparison\n",
    "- 04_roc_curves_comparison.png: ROC curves for all models\n",
    "- 05_f1_score_ranking.png: Model ranking by F1-score\n",
    "- 06_metrics_heatmap.png: Metrics heatmap visualization\n",
    "\n",
    "Model-Specific Outputs:\n",
    "\"\"\"\n",
    "\n",
    "for model_key in MODELS_TO_RUN:\n",
    "    model_dir = os.path.join(RUN_DIR, model_key)\n",
    "    if os.path.exists(model_dir):\n",
    "        files = os.listdir(model_dir)\n",
    "        summary_text += f\"\\n  {model_key.upper()}:\\n\"\n",
    "        for file in sorted(files):\n",
    "            summary_text += f\"    - {file}\\n\"\n",
    "\n",
    "summary_text += f\"\"\"\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "# Save summary to file\n",
    "summary_path = os.path.join(RUN_DIR, \"00_EXECUTION_SUMMARY.txt\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(summary_text)\n",
    "print(f\"Summary saved to: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
