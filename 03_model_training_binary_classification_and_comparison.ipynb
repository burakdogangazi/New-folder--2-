{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2c44e7",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f6b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Scikit-Learn - Model Selection\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "\n",
    "# Scikit-Learn - Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Scikit-Learn - Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Visualization Settings\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "MODELS_DIR = os.path.join(\"data\", \"models\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8e08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_method(method):\n",
    "    \"\"\"Load train/test engineered CSVs for a given method (raw/pca/ica).\n",
    "\n",
    "    Returns: (train_df, test_df)\n",
    "    \"\"\"\n",
    "    method_dir = os.path.join(\"data\", \"features\", method)\n",
    "    train_path = os.path.join(method_dir, f\"combined_engineered_{method}_train.csv\")\n",
    "    test_path = os.path.join(method_dir, f\"combined_engineered_{method}_test.csv\")\n",
    "\n",
    "    if not os.path.exists(train_path):\n",
    "        raise FileNotFoundError(f\"Train file for method '{method}' not found at: {train_path}\")\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path) if os.path.exists(test_path) else pd.DataFrame()\n",
    "\n",
    "    if test_df.empty:\n",
    "        print(f\"Warning: test file missing for method '{method}'. A 30% stratified holdout will be created from train.\")\n",
    "    print(f\"Loaded {method} → train: {train_df.shape}, test: {test_df.shape if not test_df.empty else 'missing'}\")\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def available_feature_methods():\n",
    "    base = os.path.join(\"data\", \"features\")\n",
    "    methods = []\n",
    "    for m in (\"raw\", \"pca\", \"ica\"):\n",
    "        p = os.path.join(base, m, f\"combined_engineered_{m}_train.csv\")\n",
    "        if os.path.exists(p):\n",
    "            methods.append(m)\n",
    "    print(f\"Available methods with train files: {methods}\")\n",
    "    return methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9871c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected FEATURE_METHOD: ica\n",
      "Available methods with train files: ['raw', 'pca', 'ica']\n",
      "Available on disk: ['raw', 'pca', 'ica']\n",
      "Loaded ica → train: (194926, 24), test: (83540, 24)\n",
      "Using provided test_df → X_test: (83540, 19), y_test: (83540,)\n",
      "Final shapes → X_train: (194926, 19), y_train: (194926,), X_test: (83540, 19), y_test: (83540,)\n"
     ]
    }
   ],
   "source": [
    "# --- Select feature method (change to 'raw', 'pca' or 'ica' as needed) ---\n",
    "FEATURE_METHOD = globals().get('FEATURE_METHOD', 'ica')  # default: 'raw'\n",
    "print(f\"Selected FEATURE_METHOD: {FEATURE_METHOD}\")\n",
    "\n",
    "# Show what's available on disk\n",
    "available = available_feature_methods()\n",
    "print(f\"Available on disk: {available}\")\n",
    "\n",
    "# If user already has DataFrames in memory, prefer them; otherwise load from CSVs\n",
    "if 'train_df' in globals() and 'X_train' in globals():\n",
    "    print(\"Using existing in-memory `train_df` and `X_train` variables.\")\n",
    "else:\n",
    "    train_df, test_df = load_feature_method(FEATURE_METHOD)\n",
    "\n",
    "# Define label columns and create X/y\n",
    "label_cols = [\"label1\", \"label2\", \"label3\", \"label4\", \"label_full\"]\n",
    "if 'train_df' not in globals():\n",
    "    raise RuntimeError(\"train_df not available. Load data into notebook or choose an available FEATURE_METHOD.\")\n",
    "\n",
    "X_train = train_df.drop(columns=label_cols, errors='ignore')\n",
    "# Convert primary labels to binary 0/1 here: 0 = benign, 1 = attack\n",
    "# Treat any label equal to 'benign' (case-insensitive) as benign; everything else -> attack\n",
    "y_train = (train_df['label1'].astype(str).str.lower() != 'benign').astype(int)\n",
    "\n",
    "# Prepare test set (use provided test_df if present, otherwise stratified split)\n",
    "if 'test_df' in globals() and (not test_df.empty):\n",
    "    X_test = test_df.drop(columns=label_cols, errors='ignore')\n",
    "    y_test = (test_df['label1'].astype(str).str.lower() != 'benign').astype(int)\n",
    "    print(f\"Using provided test_df → X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "else:\n",
    "    print(\"No test_df found — creating stratified holdout from train (30%).\")\n",
    "    X_train_full = X_train.copy()\n",
    "    y_train_full = y_train.copy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.30, random_state=RANDOM_STATE, stratify=y_train_full\n",
    "    )\n",
    "    print(f\"After split → X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "# Quick sanity print\n",
    "print(f\"Final shapes → X_train: {X_train.shape}, y_train: {y_train.shape}, X_test: {X_test.shape}, y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc156dd",
   "metadata": {},
   "source": [
    "## 4. Metrics Computation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf3df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, y_prob=None, class_labels=None):\n",
    "    \"\"\"\n",
    "    Compute binary classification metrics assuming numeric 0/1 labels.\n",
    "\n",
    "    Expects:\n",
    "      - y_true: iterable of 0/1\n",
    "      - y_pred: iterable of 0/1\n",
    "      - y_prob: probabilities for the positive class (1), optional\n",
    "\n",
    "    Returns a dict with accuracy, precision, recall, specificity, f1, roc_auc and confusion_matrix.\n",
    "    \"\"\"\n",
    "    # Coerce to integer numpy arrays (expect 0/1)\n",
    "    y_true_int = np.array(y_true).astype(int)\n",
    "    y_pred_int = np.array(y_pred).astype(int)\n",
    "\n",
    "    # Basic metrics\n",
    "    acc = accuracy_score(y_true_int, y_pred_int)\n",
    "    prec = precision_score(y_true_int, y_pred_int, zero_division=0)\n",
    "    rec = recall_score(y_true_int, y_pred_int, zero_division=0)\n",
    "    f1 = f1_score(y_true_int, y_pred_int, zero_division=0)\n",
    "\n",
    "    # Confusion matrix and specificity\n",
    "    cm = confusion_matrix(y_true_int, y_pred_int)\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    else:\n",
    "        spec = np.nan\n",
    "\n",
    "    # ROC-AUC using provided probabilities for positive class (1)\n",
    "    if y_prob is not None:\n",
    "        y_prob_arr = np.array(y_prob)\n",
    "        y_true_bin = (y_true_int == 1).astype(int)\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true_bin, y_prob_arr)\n",
    "        except Exception:\n",
    "            roc_auc = np.nan\n",
    "    else:\n",
    "        roc_auc = np.nan\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"specificity\": spec,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"confusion_matrix\": cm\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13bcb3b",
   "metadata": {},
   "source": [
    "## 5. Model Definition and Hyperparameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372e3458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to train: ['log_reg', 'rf', 'lgb', 'mlp', 'knn', 'nb']\n",
      "Optimization method: RandomizedSearchCV\n",
      "Cross-validation folds: 3\n",
      "Iterations per model: 10\n",
      "Total models: 6\n"
     ]
    }
   ],
   "source": [
    "model_defs = {}\n",
    "\n",
    "# ============================================================================\n",
    "# LINEAR MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Logistic Regression - Fast baseline linear classifier\n",
    "model_defs[\"log_reg\"] = {\n",
    "    \"estimator\": LogisticRegression(max_iter=5000, random_state=RANDOM_STATE),\n",
    "    \"param_grid\": {\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"C\": [0.1, 1, 10, 100],\n",
    "        \"solver\": [\"lbfgs\", \"saga\"],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# TREE-BASED MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Random Forest - Ensemble of decision trees\n",
    "model_defs[\"rf\"] = {\n",
    "    \"estimator\": RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    \"param_grid\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [10, 20, 30, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\"],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# LightGBM - Fast gradient boosting classifier\n",
    "model_defs[\"lgb\"] = {\n",
    "    \"estimator\": lgb.LGBMClassifier(\n",
    "        random_state=RANDOM_STATE, \n",
    "        n_jobs=-1, \n",
    "        verbose=-1,\n",
    "        is_unbalance=True\n",
    "    ),\n",
    "    \"param_grid\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [5, 10, 15, -1],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"num_leaves\": [20, 30, 40, 50],\n",
    "        \"min_data_in_leaf\": [10, 20, 30],\n",
    "        \"feature_fraction\": [0.8, 0.9, 1.0],\n",
    "        \"bagging_fraction\": [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# NEURAL NETWORK MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Multi-Layer Perceptron - Neural network classifier\n",
    "model_defs[\"mlp\"] = {\n",
    "    \"estimator\": MLPClassifier(random_state=RANDOM_STATE, max_iter=500, early_stopping=True),\n",
    "    \"param_grid\": {\n",
    "        \"hidden_layer_sizes\": [(64,), (128,), (64, 32), (128, 64)],\n",
    "        \"activation\": [\"relu\", \"tanh\"],\n",
    "        \"alpha\": [0.0001, 0.001, 0.01],\n",
    "        \"learning_rate\": [\"constant\", \"adaptive\"],\n",
    "        \"batch_size\": [32, 64]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# DISTANCE-BASED & PROBABILISTIC MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# K-Nearest Neighbors - Distance-based instance classifier\n",
    "model_defs[\"knn\"] = {\n",
    "    \"estimator\": KNeighborsClassifier(),\n",
    "    \"param_grid\": {\n",
    "        \"n_neighbors\": [3, 5, 7, 9, 11],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"metric\": [\"euclidean\", \"manhattan\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Gaussian Naive Bayes - Probabilistic classifier\n",
    "model_defs[\"nb\"] = {\n",
    "    \"estimator\": GaussianNB(),\n",
    "    \"param_grid\": {\n",
    "        \"var_smoothing\": np.logspace(-10, -6, 5)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODELS_TO_RUN = [\"log_reg\", \"rf\", \"lgb\", \"mlp\", \"knn\", \"nb\"]\n",
    "\n",
    "print(f\"Models to train: {MODELS_TO_RUN}\")\n",
    "print(f\"Optimization method: RandomizedSearchCV\")\n",
    "print(f\"Cross-validation folds: 3\")\n",
    "print(f\"Iterations per model: 10\")\n",
    "print(f\"Total models: {len(MODELS_TO_RUN)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4143fe",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8352d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_key, model_def, X_train, X_test, y_train, y_test,\n",
    "                             base_dir, cv_folds=3, n_iter=10, class_labels=None):\n",
    "    \"\"\"\n",
    "    Train a model using RandomizedSearchCV and evaluate on test set.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_key : str\n",
    "        Model identifier\n",
    "    model_def : dict\n",
    "        Dictionary containing estimator and parameter grid\n",
    "    X_train, X_test : array-like\n",
    "        Training and test feature matrices\n",
    "    y_train, y_test : array-like\n",
    "        Training and test labels (expected 0/1)\n",
    "    base_dir : str\n",
    "        Base directory for saving results\n",
    "    cv_folds : int\n",
    "        Number of cross-validation folds\n",
    "    n_iter : int\n",
    "        Number of RandomizedSearchCV iterations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (results_dict, best_model, predictions, probabilities, metrics)\n",
    "    \"\"\"\n",
    "    estimator = model_def[\"estimator\"]\n",
    "    param_grid = model_def[\"param_grid\"]\n",
    "    \n",
    "    MODEL_DIR = os.path.join(base_dir, model_key)\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_key.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Perform hyperparameter tuning using RandomizedSearchCV\n",
    "    if param_grid:\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            scoring=\"f1\",\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            verbose=1\n",
    "        )\n",
    "        search.fit(X_train, y_train)\n",
    "        best_model = search.best_estimator_\n",
    "        best_params = search.best_params_\n",
    "        cv_f1 = search.best_score_\n",
    "    else:\n",
    "        best_model = estimator\n",
    "        best_model.fit(X_train, y_train)\n",
    "        best_params = {}\n",
    "        cv_f1 = np.nan\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    if not np.isnan(cv_f1):\n",
    "        print(f\"Cross-validation F1 score: {cv_f1:.4f}\")\n",
    "    else:\n",
    "        print(f\"Cross-validation F1 score: N/A\")\n",
    "    \n",
    "    # Generate predictions on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Extract probability estimates for ROC curve\n",
    "    if hasattr(best_model, \"predict_proba\"):\n",
    "        y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(best_model, \"decision_function\"):\n",
    "        y_prob = best_model.decision_function(X_test)\n",
    "        y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min() + 1e-10)\n",
    "    else:\n",
    "        y_prob = y_pred.astype(float)\n",
    "    \n",
    "    # Compute comprehensive metrics (expect numeric 0/1 labels)\n",
    "    labels_names = [\"Benign\", \"Attack\"]\n",
    "    metrics = compute_metrics(y_test, y_pred, y_prob)\n",
    "    \n",
    "    # Generate and save confusion matrix visualization\n",
    "    cm = metrics[\"confusion_matrix\"]\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=labels_names, yticklabels=labels_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(f\"{model_key.upper()} - Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    cm_path = os.path.join(MODEL_DIR, f\"{model_key}_confusion_matrix.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Generate and save ROC curve visualization\n",
    "    # For ROC we need binary true labels (0/1)\n",
    "    y_test_bin = np.array(y_test).astype(int)\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin, y_prob)\n",
    "    roc_auc = roc_auc_score(y_test_bin, y_prob)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2.5, label=f\"AUC = {roc_auc:.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw=1.5, label=\"Random Classifier\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=11)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=11)\n",
    "    plt.title(f\"ROC Curve - {model_key.upper()}\", fontsize=12)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    roc_path = os.path.join(MODEL_DIR, f\"{model_key}_roc_curve.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(roc_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Visualizations saved for {model_key}\")\n",
    "    \n",
    "    # Return summary metrics (include best_model for saving later)\n",
    "    return {\n",
    "        \"model\": model_key,\n",
    "        \"best_params\": str(best_params),\n",
    "        \"cv_f1_score\": cv_f1,\n",
    "        \"test_accuracy\": metrics[\"accuracy\"],\n",
    "        \"test_precision\": metrics[\"precision\"],\n",
    "        \"test_recall\": metrics[\"recall\"],\n",
    "        \"test_specificity\": metrics[\"specificity\"],\n",
    "        \"test_f1\": metrics[\"f1\"],\n",
    "        \"test_roc_auc\": metrics[\"roc_auc\"]\n",
    "    }, best_model, y_pred, y_prob, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc6dee",
   "metadata": {},
   "source": [
    "## 7. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfbdba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels read from data: ['0', '1']; positive_label: 1\n",
      "Results will be saved to: binary_classification\\ica\\results_20251217_194002\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training LOG_REG\n",
      "============================================================\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'class_weight': 'balanced', 'C': 100}\n",
      "Cross-validation F1 score: 0.8686\n",
      "Visualizations saved for log_reg\n",
      "\n",
      "============================================================\n",
      "Training RF\n",
      "============================================================\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'n_estimators': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': 'balanced'}\n",
      "Cross-validation F1 score: 0.9485\n",
      "Visualizations saved for rf\n",
      "\n",
      "============================================================\n",
      "Training LGB\n",
      "============================================================\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'num_leaves': 40, 'n_estimators': 300, 'min_data_in_leaf': 30, 'max_depth': -1, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 0.8}\n",
      "Cross-validation F1 score: 0.9482\n",
      "Visualizations saved for lgb\n",
      "\n",
      "============================================================\n",
      "Training MLP\n",
      "============================================================\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'learning_rate': 'adaptive', 'hidden_layer_sizes': (128, 64), 'batch_size': 64, 'alpha': 0.001, 'activation': 'tanh'}\n",
      "Cross-validation F1 score: 0.9390\n",
      "Visualizations saved for mlp\n",
      "\n",
      "============================================================\n",
      "Training KNN\n",
      "============================================================\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'weights': 'distance', 'n_neighbors': 7, 'metric': 'manhattan'}\n",
      "Cross-validation F1 score: 0.9460\n",
      "Visualizations saved for knn\n",
      "\n",
      "============================================================\n",
      "Training NB\n",
      "============================================================\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 5 is smaller than n_iter=10. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'var_smoothing': np.float64(1e-10)}\n",
      "Cross-validation F1 score: 0.6369\n",
      "Visualizations saved for nb\n",
      "\n",
      "============================================================\n",
      "MODEL TRAINING COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "Metrics summary saved: binary_classification\\ica\\results_20251217_194002\\01_metrics_summary_all_models.csv\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>cv_f1_score</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_specificity</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>{'solver': 'lbfgs', 'penalty': 'l2', 'class_we...</td>\n",
       "      <td>0.868553</td>\n",
       "      <td>0.879136</td>\n",
       "      <td>0.952459</td>\n",
       "      <td>0.798109</td>\n",
       "      <td>0.960163</td>\n",
       "      <td>0.868479</td>\n",
       "      <td>0.923204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf</td>\n",
       "      <td>{'n_estimators': 50, 'min_samples_split': 5, '...</td>\n",
       "      <td>0.948527</td>\n",
       "      <td>0.951006</td>\n",
       "      <td>0.988918</td>\n",
       "      <td>0.912234</td>\n",
       "      <td>0.989777</td>\n",
       "      <td>0.949029</td>\n",
       "      <td>0.977716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lgb</td>\n",
       "      <td>{'num_leaves': 40, 'n_estimators': 300, 'min_d...</td>\n",
       "      <td>0.948238</td>\n",
       "      <td>0.949521</td>\n",
       "      <td>0.991287</td>\n",
       "      <td>0.907015</td>\n",
       "      <td>0.992028</td>\n",
       "      <td>0.947280</td>\n",
       "      <td>0.977067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mlp</td>\n",
       "      <td>{'learning_rate': 'adaptive', 'hidden_layer_si...</td>\n",
       "      <td>0.938952</td>\n",
       "      <td>0.941286</td>\n",
       "      <td>0.988809</td>\n",
       "      <td>0.892674</td>\n",
       "      <td>0.989897</td>\n",
       "      <td>0.938286</td>\n",
       "      <td>0.968328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'weights': 'distance', 'n_neighbors': 7, 'met...</td>\n",
       "      <td>0.946013</td>\n",
       "      <td>0.949461</td>\n",
       "      <td>0.988779</td>\n",
       "      <td>0.909241</td>\n",
       "      <td>0.989682</td>\n",
       "      <td>0.947343</td>\n",
       "      <td>0.972049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nb</td>\n",
       "      <td>{'var_smoothing': np.float64(1e-10)}</td>\n",
       "      <td>0.636936</td>\n",
       "      <td>0.725952</td>\n",
       "      <td>0.943473</td>\n",
       "      <td>0.480704</td>\n",
       "      <td>0.971199</td>\n",
       "      <td>0.636903</td>\n",
       "      <td>0.885040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model                                        best_params  cv_f1_score  \\\n",
       "0  log_reg  {'solver': 'lbfgs', 'penalty': 'l2', 'class_we...     0.868553   \n",
       "1       rf  {'n_estimators': 50, 'min_samples_split': 5, '...     0.948527   \n",
       "2      lgb  {'num_leaves': 40, 'n_estimators': 300, 'min_d...     0.948238   \n",
       "3      mlp  {'learning_rate': 'adaptive', 'hidden_layer_si...     0.938952   \n",
       "4      knn  {'weights': 'distance', 'n_neighbors': 7, 'met...     0.946013   \n",
       "5       nb               {'var_smoothing': np.float64(1e-10)}     0.636936   \n",
       "\n",
       "   test_accuracy  test_precision  test_recall  test_specificity   test_f1  \\\n",
       "0       0.879136        0.952459     0.798109          0.960163  0.868479   \n",
       "1       0.951006        0.988918     0.912234          0.989777  0.949029   \n",
       "2       0.949521        0.991287     0.907015          0.992028  0.947280   \n",
       "3       0.941286        0.988809     0.892674          0.989897  0.938286   \n",
       "4       0.949461        0.988779     0.909241          0.989682  0.947343   \n",
       "5       0.725952        0.943473     0.480704          0.971199  0.636903   \n",
       "\n",
       "   test_roc_auc  \n",
       "0      0.923204  \n",
       "1      0.977716  \n",
       "2      0.977067  \n",
       "3      0.968328  \n",
       "4      0.972049  \n",
       "5      0.885040  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BEST MODEL: RF\n",
      "Test F1-Score: 0.9490\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate timestamp for unique results directory\n",
    "\n",
    "# Read class labels directly from training data (do not hardcode)\n",
    "class_labels = sorted(np.unique(y_train.astype(str)))\n",
    "if len(class_labels) != 2:\n",
    "    raise ValueError(f\"Expected binary labels in 'label1' but found {len(class_labels)} classes: {class_labels}\")\n",
    "# Define positive label for ROC as the second label in sorted order\n",
    "positive_label = class_labels[1]\n",
    "print(f\"Class labels read from data: {class_labels}; positive_label: {positive_label}\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = os.path.join(\"binary_classification\", FEATURE_METHOD, f\"results_{timestamp}\")\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved to: {RUN_DIR}\\n\")\n",
    "\n",
    "results_list = []\n",
    "models_dict = {}\n",
    "y_preds_dict = {}\n",
    "y_probs_dict = {}\n",
    "metrics_dict = {}\n",
    "\n",
    "# Train all models using optimized RandomizedSearchCV\n",
    "for key in MODELS_TO_RUN:\n",
    "    result_row, best_model, y_pred, y_prob, metrics = train_and_evaluate_model(\n",
    "        model_key=key,\n",
    "        model_def=model_defs[key],\n",
    "        X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test,\n",
    "        base_dir=RUN_DIR,\n",
    "        cv_folds=3,\n",
    "        n_iter=10,\n",
    "        class_labels=class_labels\n",
    "    )\n",
    "    results_list.append(result_row)\n",
    "    models_dict[key] = best_model\n",
    "    y_preds_dict[key] = y_pred\n",
    "    y_probs_dict[key] = y_prob\n",
    "    metrics_dict[key] = metrics\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL TRAINING COMPLETED SUCCESSFULLY\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Create metrics summary dataframe\n",
    "metrics_df = pd.DataFrame(results_list)\n",
    "metrics_csv = os.path.join(RUN_DIR, \"01_metrics_summary_all_models.csv\")\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "print(f\"Metrics summary saved: {metrics_csv}\\n\")\n",
    "display(metrics_df)\n",
    "\n",
    "# Identify best performing model\n",
    "best_model_key = metrics_df.loc[metrics_df[\"test_f1\"].idxmax(), \"model\"]\n",
    "best_model_f1 = metrics_df.loc[metrics_df[\"test_f1\"].idxmax(), \"test_f1\"]\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL: {best_model_key.upper()}\")\n",
    "print(f\"Test F1-Score: {best_model_f1:.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9590c420",
   "metadata": {},
   "source": [
    "## 8. Best Model Detailed Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52aa2506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved: binary_classification\\ica\\results_20251217_194002\\best_binary_model_rf.pkl\n",
      "\n",
      "Best model also saved to production: data\\models\\best_binary_classification_model.pkl\n",
      "\n",
      "Best model report generated and saved\n",
      "\n",
      "======================================================================\n",
      "BINARY INTRUSION DETECTION - BEST MODEL REPORT\n",
      "======================================================================\n",
      "\n",
      "Execution Timestamp: 20251217_194002\n",
      "Best Model: RF\n",
      "Results Location: binary_classification\\ica\\results_20251217_194002\\best_binary_model_rf.pkl\n",
      "Production Location: data\\models\\best_binary_classification_model.pkl\n",
      "\n",
      "======================================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "======================================================================\n",
      "\n",
      "Accuracy:       0.9510\n",
      "Precision:      0.9889\n",
      "Recall:         0.9122\n",
      "Specificity:    0.9898\n",
      "F1-Score:       0.9490\n",
      "ROC-AUC:        0.9777\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41343   427]\n",
      " [ 3666 38104]]\n",
      "\n",
      "======================================================================\n",
      "CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign     0.9185    0.9898    0.9528     41770\n",
      "      Attack     0.9889    0.9122    0.9490     41770\n",
      "\n",
      "    accuracy                         0.9510     83540\n",
      "   macro avg     0.9537    0.9510    0.9509     83540\n",
      "weighted avg     0.9537    0.9510    0.9509     83540\n",
      "\n",
      "\n",
      "======================================================================\n",
      "BEST MODEL HYPERPARAMETERS\n",
      "======================================================================\n",
      "\n",
      "{'n_estimators': [50, 100, 200], 'max_depth': [10, 20, 30, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'max_features': ['sqrt'], 'class_weight': [None, 'balanced']}\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract best model results\n",
    "best_model_obj = models_dict[best_model_key]\n",
    "y_pred_best = y_preds_dict[best_model_key]\n",
    "y_prob_best = y_probs_dict[best_model_key]\n",
    "metrics_best = metrics_dict[best_model_key]\n",
    "\n",
    "# Save best model to its results directory\n",
    "best_model_path = os.path.join(RUN_DIR, f\"best_binary_model_{best_model_key}.pkl\")\n",
    "joblib.dump(best_model_obj, best_model_path)\n",
    "print(f\"Best model saved: {best_model_path}\\n\")\n",
    "\n",
    "# Also save to production directory for deployment\n",
    "best_model_prod_path = os.path.join(MODELS_DIR, f\"best_binary_classification_model.pkl\")\n",
    "joblib.dump(best_model_obj, best_model_prod_path)\n",
    "print(f\"Best model also saved to production: {best_model_prod_path}\\n\")\n",
    "\n",
    "# Generate classification report for best model\n",
    "class_report = classification_report(y_test, y_pred_best,\n",
    "                                     target_names=[\"Benign\", \"Attack\"],\n",
    "                                     digits=4)\n",
    "\n",
    "# Create detailed report text\n",
    "report_text = f\"\"\"\n",
    "{'='*70}\n",
    "BINARY INTRUSION DETECTION - BEST MODEL REPORT\n",
    "{'='*70}\n",
    "\n",
    "Execution Timestamp: {timestamp}\n",
    "Best Model: {best_model_key.upper()}\n",
    "Results Location: {best_model_path}\n",
    "Production Location: {best_model_prod_path}\n",
    "\n",
    "{'='*70}\n",
    "MODEL PERFORMANCE METRICS\n",
    "{'='*70}\n",
    "\n",
    "Accuracy:       {metrics_best['accuracy']:.4f}\n",
    "Precision:      {metrics_best['precision']:.4f}\n",
    "Recall:         {metrics_best['recall']:.4f}\n",
    "Specificity:    {metrics_best['specificity']:.4f}\n",
    "F1-Score:       {metrics_best['f1']:.4f}\n",
    "ROC-AUC:        {metrics_best['roc_auc']:.4f}\n",
    "\n",
    "Confusion Matrix:\n",
    "{metrics_best['confusion_matrix']}\n",
    "\n",
    "{'='*70}\n",
    "CLASSIFICATION REPORT\n",
    "{'='*70}\n",
    "\n",
    "{class_report}\n",
    "\n",
    "{'='*70}\n",
    "BEST MODEL HYPERPARAMETERS\n",
    "{'='*70}\n",
    "\n",
    "{str(model_defs[best_model_key]['param_grid'])}\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "# Save report to file\n",
    "report_path = os.path.join(RUN_DIR, \"02_best_model_report.txt\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(\"Best model report generated and saved\")\n",
    "print(report_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ddbec",
   "metadata": {},
   "source": [
    "## 9. Model Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d1f43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: models_comparison_metrics.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1201: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1201: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1201: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1201: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1201: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1201: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\burak.dogan.2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: roc_curves_comparison.png\n",
      "Saved: f1_score_ranking.png\n",
      "Saved: metrics_heatmap.png\n",
      "\n",
      "All comparison visualizations generated successfully\n"
     ]
    }
   ],
   "source": [
    "# 1. Performance metrics comparison across models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle(\"Model Performance Comparison\", fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = [\"test_accuracy\", \"test_precision\", \"test_recall\",\n",
    "                   \"test_specificity\", \"test_f1\", \"test_roc_auc\"]\n",
    "colors = ['#FF6B6B' if model == best_model_key else '#4ECDC4'\n",
    "          for model in metrics_df[\"model\"]]\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    bars = ax.bar(metrics_df[\"model\"], metrics_df[metric], color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax.set_ylabel(metric.replace(\"test_\", \"\"), fontsize=10)\n",
    "    ax.set_title(metric.replace(\"test_\", \"\").upper(), fontsize=11, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_path = os.path.join(RUN_DIR, \"03_models_comparison_metrics.png\")\n",
    "plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: models_comparison_metrics.png\")\n",
    "\n",
    "# 2. ROC curves comparison\n",
    "plt.figure(figsize=(10, 8))\n",
    "for model_key in MODELS_TO_RUN:\n",
    "    y_prob = y_probs_dict[model_key]\n",
    "    # Convert string labels to binary using positive_label\n",
    "    y_test_bin = (np.array(y_test) == positive_label).astype(int)\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin, y_prob)\n",
    "    auc = roc_auc_score(y_test_bin, y_prob)\n",
    "\n",
    "    line_width = 3 if model_key == best_model_key else 1.5\n",
    "    line_style = '-' if model_key == best_model_key else '--'\n",
    "    alpha = 1.0 if model_key == best_model_key else 0.7\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=line_width, linestyle=line_style, alpha=alpha,\n",
    "             label=f\"{model_key.upper()} (AUC={auc:.4f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "plt.title(\"ROC Curves - All Models Comparison\", fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "roc_comparison_path = os.path.join(RUN_DIR, \"04_roc_curves_comparison.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(roc_comparison_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: roc_curves_comparison.png\")\n",
    "\n",
    "# 3. F1-score ranking\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sorted_df = metrics_df.sort_values(\"test_f1\", ascending=True)\n",
    "colors_rank = ['#FF6B6B' if model == best_model_key else '#95E1D3'\n",
    "               for model in sorted_df[\"model\"]]\n",
    "bars = ax.barh(sorted_df[\"model\"], sorted_df[\"test_f1\"], color=colors_rank, edgecolor='black', alpha=0.85)\n",
    "\n",
    "ax.set_xlabel(\"F1-Score\", fontsize=12)\n",
    "ax.set_title(\"Models Ranked by F1-Score\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([0, 1.05])\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for idx, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "            f' {width:.4f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "ranking_path = os.path.join(RUN_DIR, \"05_f1_score_ranking.png\")\n",
    "plt.savefig(ranking_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: f1_score_ranking.png\")\n",
    "\n",
    "# 4. Metrics heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "metrics_for_heatmap = metrics_df[[\"model\", \"test_accuracy\", \"test_precision\",\n",
    "                                   \"test_recall\", \"test_specificity\", \"test_f1\", \"test_roc_auc\"]].set_index(\"model\")\n",
    "sns.heatmap(metrics_for_heatmap.T, annot=True, fmt='.4f', cmap='RdYlGn',\n",
    "            cbar_kws={'label': 'Score'}, ax=ax, linewidths=0.5)\n",
    "ax.set_title(\"Model Metrics Heatmap\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "heatmap_path = os.path.join(RUN_DIR, \"06_metrics_heatmap.png\")\n",
    "plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: metrics_heatmap.png\")\n",
    "\n",
    "print(\"\\nAll comparison visualizations generated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b912cdc",
   "metadata": {},
   "source": [
    "## 10. Execution Summary and Results Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0333236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESULTS DIRECTORY STRUCTURE\n",
      "======================================================================\n",
      "\n",
      "results_20251217_194002/\n",
      "  01_metrics_summary_all_models.csv (1.5KB)\n",
      "  02_best_model_report.txt (1.7KB)\n",
      "  03_models_comparison_metrics.png (291.6KB)\n",
      "  04_roc_curves_comparison.png (188.5KB)\n",
      "  05_f1_score_ranking.png (87.8KB)\n",
      "  06_metrics_heatmap.png (252.3KB)\n",
      "  best_binary_model_rf.pkl (19358.4KB)\n",
      "  knn/\n",
      "    knn_confusion_matrix.png (82.4KB)\n",
      "    knn_roc_curve.png (104.7KB)\n",
      "  lgb/\n",
      "    lgb_confusion_matrix.png (84.4KB)\n",
      "    lgb_roc_curve.png (104.6KB)\n",
      "  log_reg/\n",
      "    log_reg_confusion_matrix.png (86.7KB)\n",
      "    log_reg_roc_curve.png (112.8KB)\n",
      "  mlp/\n",
      "    mlp_confusion_matrix.png (82.1KB)\n",
      "    mlp_roc_curve.png (107.0KB)\n",
      "  nb/\n",
      "    nb_confusion_matrix.png (84.1KB)\n",
      "    nb_roc_curve.png (113.4KB)\n",
      "  rf/\n",
      "    rf_confusion_matrix.png (82.4KB)\n",
      "    rf_roc_curve.png (103.1KB)\n",
      "\n",
      "======================================================================\n",
      "EXECUTION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "\n",
      "BINARY CLASSIFICATION MODEL TRAINING SUMMARY\n",
      "\n",
      "Total Models Trained: 6\n",
      "Models: LOG_REG, RF, LGB, MLP, KNN, NB\n",
      "\n",
      "Best Performing Model: RF\n",
      "Best F1-Score: 0.9490\n",
      "\n",
      "Results Location: c:\\Users\\burak.dogan.2\\Desktop\\Projeler\\SC\\AI-IDS\\binary_classification\\ica\\results_20251217_194002\n",
      "\n",
      "Generated Files:\n",
      "- 01_metrics_summary_all_models.csv: Comprehensive metrics for all models\n",
      "- 02_best_model_report.txt: Detailed analysis of the best model\n",
      "- 03_models_comparison_metrics.png: Performance metrics comparison\n",
      "- 04_roc_curves_comparison.png: ROC curves for all models\n",
      "- 05_f1_score_ranking.png: Model ranking by F1-score\n",
      "- 06_metrics_heatmap.png: Metrics heatmap visualization\n",
      "\n",
      "Model-Specific Outputs:\n",
      "\n",
      "  LOG_REG:\n",
      "    - log_reg_confusion_matrix.png\n",
      "    - log_reg_roc_curve.png\n",
      "\n",
      "  RF:\n",
      "    - rf_confusion_matrix.png\n",
      "    - rf_roc_curve.png\n",
      "\n",
      "  LGB:\n",
      "    - lgb_confusion_matrix.png\n",
      "    - lgb_roc_curve.png\n",
      "\n",
      "  MLP:\n",
      "    - mlp_confusion_matrix.png\n",
      "    - mlp_roc_curve.png\n",
      "\n",
      "  KNN:\n",
      "    - knn_confusion_matrix.png\n",
      "    - knn_roc_curve.png\n",
      "\n",
      "  NB:\n",
      "    - nb_confusion_matrix.png\n",
      "    - nb_roc_curve.png\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Summary saved to: binary_classification\\ica\\results_20251217_194002\\00_EXECUTION_SUMMARY.txt\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RESULTS DIRECTORY STRUCTURE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# List all generated files in results directory\n",
    "for root, dirs, files in os.walk(RUN_DIR):\n",
    "    level = root.replace(RUN_DIR, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    sub_indent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        size_str = f\"{file_size / 1024:.1f}KB\" if file_size > 1024 else f\"{file_size}B\"\n",
    "        print(f'{sub_indent}{file} ({size_str})')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Generate execution summary\n",
    "summary_text = f\"\"\"\n",
    "BINARY CLASSIFICATION MODEL TRAINING SUMMARY\n",
    "\n",
    "Total Models Trained: {len(MODELS_TO_RUN)}\n",
    "Models: {', '.join([m.upper() for m in MODELS_TO_RUN])}\n",
    "\n",
    "Best Performing Model: {best_model_key.upper()}\n",
    "Best F1-Score: {best_model_f1:.4f}\n",
    "\n",
    "Results Location: {os.path.abspath(RUN_DIR)}\n",
    "\n",
    "Generated Files:\n",
    "- 01_metrics_summary_all_models.csv: Comprehensive metrics for all models\n",
    "- 02_best_model_report.txt: Detailed analysis of the best model\n",
    "- 03_models_comparison_metrics.png: Performance metrics comparison\n",
    "- 04_roc_curves_comparison.png: ROC curves for all models\n",
    "- 05_f1_score_ranking.png: Model ranking by F1-score\n",
    "- 06_metrics_heatmap.png: Metrics heatmap visualization\n",
    "\n",
    "Model-Specific Outputs:\n",
    "\"\"\"\n",
    "\n",
    "for model_key in MODELS_TO_RUN:\n",
    "    model_dir = os.path.join(RUN_DIR, model_key)\n",
    "    if os.path.exists(model_dir):\n",
    "        files = os.listdir(model_dir)\n",
    "        summary_text += f\"\\n  {model_key.upper()}:\\n\"\n",
    "        for file in sorted(files):\n",
    "            summary_text += f\"    - {file}\\n\"\n",
    "\n",
    "summary_text += f\"\"\"\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "# Save summary to file\n",
    "summary_path = os.path.join(RUN_DIR, \"00_EXECUTION_SUMMARY.txt\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(summary_text)\n",
    "print(f\"Summary saved to: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
