{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ed2994",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd9775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Model Selection & Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    roc_curve, auc, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Display Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d4096c",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab684283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature-engineered dataset\n",
    "df = pd.read_csv(FEATURE_PATH)\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Validate presence of required label columns\n",
    "assert \"label1\" in df.columns, \"label1 column not found in dataset\"\n",
    "assert \"label2\" in df.columns, \"label2 column not found in dataset\"\n",
    "\n",
    "# Filter to retain only attack traffic (exclude benign)\n",
    "df_attacks = df[df[\"label1\"].str.lower() == \"attack\"].copy()\n",
    "print(f\"\\nAttack traffic records: {df_attacks.shape[0]}\")\n",
    "print(f\"Percentage of attack traffic: {(df_attacks.shape[0] / df.shape[0] * 100):.2f}%\")\n",
    "\n",
    "# Analyze attack type distribution\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Attack Type Distribution (Label2)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(df_attacks[\"label2\"].value_counts())\n",
    "print(f\"\\nNumber of attack classes: {df_attacks['label2'].nunique()}\")\n",
    "\n",
    "# Extract features and create multiclass target variable\n",
    "# Target: label2 (attack sub-types)\n",
    "X = df_attacks.drop(columns=[\"label1\", \"label2\", \"label3\", \"label4\", \"label_full\"], errors='ignore')\n",
    "y = df_attacks[\"label2\"].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target variable shape: {y.shape}\")\n",
    "print(f\"\\nClass distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8fd5ba",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "# Perform stratified train-test split to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"\\nTraining set class distribution:\\n{y_train.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33e9b0",
   "metadata": {},
   "source": [
    "## 4. Multiclass Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_defs = {}\n",
    "\n",
    "# ============================================================================\n",
    "# LINEAR MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Logistic Regression - Multi-class linear classifier\n",
    "model_defs[\"log_reg\"] = {\n",
    "    \"estimator\": LogisticRegression(max_iter=5000, random_state=RANDOM_STATE, multi_class=\"multinomial\"),\n",
    "    \"param_grid\": {\n",
    "        \"penalty\": [\"l2\",\"elasticnet\"],\n",
    "        \"C\": [0.1, 1, 10, 100],\n",
    "        \"solver\": [\"lbfgs\", \"saga\"],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# TREE-BASED MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Random Forest - Ensemble of decision trees\n",
    "model_defs[\"rf\"] = {\n",
    "    \"estimator\": RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    \"param_grid\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [10, 20, 30, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\"],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# LightGBM - Fast gradient boosting classifier\n",
    "model_defs[\"lgb\"] = {\n",
    "    \"estimator\": lgb.LGBMClassifier(\n",
    "        random_state=RANDOM_STATE, \n",
    "        n_jobs=-1, \n",
    "        verbose=-1,\n",
    "        is_unbalance=True\n",
    "    ),\n",
    "    \"param_grid\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [5, 10, 15, -1],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"num_leaves\": [20, 30, 40, 50],\n",
    "        \"min_data_in_leaf\": [10, 20, 30],\n",
    "        \"feature_fraction\": [0.8, 0.9, 1.0],\n",
    "        \"bagging_fraction\": [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# NEURAL NETWORK MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Multi-Layer Perceptron - Neural network classifier for multi-class\n",
    "model_defs[\"mlp\"] = {\n",
    "    \"estimator\": MLPClassifier(random_state=RANDOM_STATE, max_iter=500, early_stopping=True, n_jobs=-1),\n",
    "    \"param_grid\": {\n",
    "        \"hidden_layer_sizes\": [(64,), (128,), (64, 32), (128, 64)],\n",
    "        \"activation\": [\"relu\", \"tanh\"],\n",
    "        \"alpha\": [0.0001, 0.001, 0.01],\n",
    "        \"learning_rate\": [\"constant\", \"adaptive\"],\n",
    "        \"batch_size\": [32, 64]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# DISTANCE-BASED & PROBABILISTIC MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# K-Nearest Neighbors - Distance-based instance classifier\n",
    "model_defs[\"knn\"] = {\n",
    "    \"estimator\": KNeighborsClassifier(),\n",
    "    \"param_grid\": {\n",
    "        \"n_neighbors\": [3, 5, 7, 9, 11],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"metric\": [\"euclidean\", \"manhattan\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Gaussian Naive Bayes - Probabilistic classifier\n",
    "model_defs[\"nb\"] = {\n",
    "    \"estimator\": GaussianNB(),\n",
    "    \"param_grid\": {\n",
    "        \"var_smoothing\": np.logspace(-10, -6, 5)\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODELS_TO_RUN = [\"log_reg\", \"rf\", \"lgb\", \"mlp\", \"knn\", \"nb\"]\n",
    "\n",
    "print(f\"Models to train: {MODELS_TO_RUN}\")\n",
    "print(f\"Optimization method: RandomizedSearchCV\")\n",
    "print(f\"Cross-validation folds: 3\")\n",
    "print(f\"Iterations per model: 10\")\n",
    "print(f\"Total models: {len(MODELS_TO_RUN)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75334acb",
   "metadata": {},
   "source": [
    "## 5. Model Definition and Hyperparameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0752bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_defs = {}\n",
    "\n",
    "# ============================================================================\n",
    "# LINEAR MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Logistic Regression - Fast baseline linear classifier for multiclass\n",
    "model_defs[\"log_reg\"] = {\n",
    "    \"estimator\": LogisticRegression(max_iter=5000, random_state=RANDOM_STATE, multi_class='multinomial'),\n",
    "    \"param_grid\": {\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"C\": [0.1, 1, 10, 100],\n",
    "        \"solver\": [\"lbfgs\", \"saga\"],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Stochastic Gradient Descent - Fast linear classifier for multiclass\n",
    "model_defs[\"sgd\"] = {\n",
    "    \"estimator\": SGDClassifier(random_state=RANDOM_STATE, n_jobs=-1, early_stopping=True, loss='log_loss'),\n",
    "    \"param_grid\": {\n",
    "        \"loss\": [\"log_loss\", \"modified_huber\"],\n",
    "        \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "        \"alpha\": [0.0001, 0.001, 0.01, 0.1],\n",
    "        \"max_iter\": [100, 200, 500],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# TREE-BASED MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Decision Tree - Interpretable tree-based classifier\n",
    "model_defs[\"dt\"] = {\n",
    "    \"estimator\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    \"param_grid\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [10, 20, 30, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Random Forest - Ensemble of decision trees\n",
    "model_defs[\"rf\"] = {\n",
    "    \"estimator\": RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    \"param_grid\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [10, 20, 30, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\"],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# LightGBM - Fast gradient boosting classifier for multiclass\n",
    "model_defs[\"lgb\"] = {\n",
    "    \"estimator\": lgb.LGBMClassifier(\n",
    "        random_state=RANDOM_STATE, \n",
    "        n_jobs=-1, \n",
    "        verbose=-1,\n",
    "        is_unbalance=True,\n",
    "        num_leaves=31\n",
    "    ),\n",
    "    \"param_grid\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [5, 10, 15, -1],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"num_leaves\": [20, 30, 40],\n",
    "        \"min_data_in_leaf\": [10, 20, 30],\n",
    "        \"feature_fraction\": [0.8, 0.9, 1.0],\n",
    "        \"bagging_fraction\": [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# DISTANCE-BASED & PROBABILISTIC MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# K-Nearest Neighbors - Distance-based instance classifier\n",
    "model_defs[\"knn\"] = {\n",
    "    \"estimator\": KNeighborsClassifier(),\n",
    "    \"param_grid\": {\n",
    "        \"n_neighbors\": [3, 5, 7, 9, 11],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"metric\": [\"euclidean\", \"manhattan\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Gaussian Naive Bayes - Probabilistic classifier\n",
    "model_defs[\"nb\"] = {\n",
    "    \"estimator\": GaussianNB(),\n",
    "    \"param_grid\": {\n",
    "        \"var_smoothing\": np.logspace(-10, -6, 5)\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODELS_TO_RUN = [\"log_reg\", \"sgd\", \"dt\", \"rf\", \"lgb\", \"knn\", \"nb\"]\n",
    "\n",
    "print(f\"Models to train: {MODELS_TO_RUN}\")\n",
    "print(f\"Optimization method: RandomizedSearchCV\")\n",
    "print(f\"Cross-validation folds: 3\")\n",
    "print(f\"Iterations per model: 10\")\n",
    "print(f\"Scoring metric: F1 (macro-averaged for multiclass)\")\n",
    "print(f\"Total models: {len(MODELS_TO_RUN)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857dba5",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618462bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_key, model_def, X_train, X_test, y_train, y_test,\n",
    "                             base_dir, cv_folds=3, n_iter=10, class_labels=None):\n",
    "    \"\"\"\n",
    "    Train a multiclass model using RandomizedSearchCV and evaluate on test set.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_key : str\n",
    "        Model identifier\n",
    "    model_def : dict\n",
    "        Dictionary containing estimator and parameter grid\n",
    "    X_train, X_test : array-like\n",
    "        Training and test feature matrices\n",
    "    y_train, y_test : array-like\n",
    "        Training and test labels\n",
    "    base_dir : str\n",
    "        Base directory for saving results\n",
    "    cv_folds : int\n",
    "        Number of cross-validation folds\n",
    "    n_iter : int\n",
    "        Number of RandomizedSearchCV iterations\n",
    "    class_labels : list\n",
    "        List of class labels for visualization\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (results_dict, best_model, predictions, metrics)\n",
    "    \"\"\"\n",
    "    estimator = model_def[\"estimator\"]\n",
    "    param_grid = model_def[\"param_grid\"]\n",
    "\n",
    "    MODEL_DIR = os.path.join(base_dir, model_key)\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_key.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Perform hyperparameter tuning using RandomizedSearchCV\n",
    "    if param_grid:\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            scoring=\"f1_macro\",\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            verbose=1\n",
    "        )\n",
    "        search.fit(X_train, y_train)\n",
    "        best_model = search.best_estimator_\n",
    "        best_params = search.best_params_\n",
    "        cv_f1 = search.best_score_\n",
    "    else:\n",
    "        best_model = estimator\n",
    "        best_model.fit(X_train, y_train)\n",
    "        best_params = {}\n",
    "        cv_f1 = np.nan\n",
    "\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Cross-validation F1 score (macro): {cv_f1:.4f}\")\n",
    "\n",
    "    # Generate predictions on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Compute comprehensive metrics\n",
    "    metrics = compute_metrics_multiclass(y_test, y_pred)\n",
    "\n",
    "    # Generate and save confusion matrix visualization\n",
    "    cm = metrics[\"confusion_matrix\"]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_labels, yticklabels=class_labels,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(f\"{model_key.upper()} - Confusion Matrix (Multiclass)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    cm_path = os.path.join(MODEL_DIR, f\"{model_key}_confusion_matrix.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Visualizations saved for {model_key}\")\n",
    "\n",
    "    # Return summary metrics\n",
    "    return {\n",
    "        \"model\": model_key,\n",
    "        \"best_params\": str(best_params),\n",
    "        \"cv_f1_macro\": cv_f1,\n",
    "        \"test_accuracy\": metrics[\"accuracy\"],\n",
    "        \"test_precision_macro\": metrics[\"precision_macro\"],\n",
    "        \"test_recall_macro\": metrics[\"recall_macro\"],\n",
    "        \"test_f1_macro\": metrics[\"f1_macro\"],\n",
    "        \"test_f1_weighted\": metrics[\"f1_weighted\"]\n",
    "    }, best_model, y_pred, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bedd408",
   "metadata": {},
   "source": [
    "## 7. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de57a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract class labels for visualization\n",
    "class_labels = sorted(y_train.unique())\n",
    "\n",
    "# Generate timestamp for unique results directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = os.path.join(\"multiclass_classification\", f\"results_{timestamp}\")\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved to: {RUN_DIR}\\n\")\n",
    "\n",
    "results_list = []\n",
    "models_dict = {}\n",
    "y_preds_dict = {}\n",
    "metrics_dict = {}\n",
    "\n",
    "# Train all models using optimized RandomizedSearchCV\n",
    "for key in MODELS_TO_RUN:\n",
    "    result_row, best_model, y_pred, metrics = train_and_evaluate_model(\n",
    "        model_key=key,\n",
    "        model_def=model_defs[key],\n",
    "        X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test,\n",
    "        base_dir=RUN_DIR,\n",
    "        cv_folds=3,\n",
    "        n_iter=10,\n",
    "        class_labels=class_labels\n",
    "    )\n",
    "    results_list.append(result_row)\n",
    "    models_dict[key] = best_model\n",
    "    y_preds_dict[key] = y_pred\n",
    "    metrics_dict[key] = metrics\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL TRAINING COMPLETED SUCCESSFULLY\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Create metrics summary dataframe\n",
    "metrics_df = pd.DataFrame(results_list)\n",
    "metrics_csv = os.path.join(RUN_DIR, \"01_metrics_summary_all_models.csv\")\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "print(f\"Metrics summary saved: {metrics_csv}\\n\")\n",
    "display(metrics_df)\n",
    "\n",
    "# Identify best performing model based on F1-macro\n",
    "best_model_key = metrics_df.loc[metrics_df[\"test_f1_macro\"].idxmax(), \"model\"]\n",
    "best_model_f1 = metrics_df.loc[metrics_df[\"test_f1_macro\"].idxmax(), \"test_f1_macro\"]\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL: {best_model_key.upper()}\")\n",
    "print(f\"Test F1-Score (Macro): {best_model_f1:.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb51f8ad",
   "metadata": {},
   "source": [
    "## 8. Best Model Detailed Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945005ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best model results\n",
    "best_model_obj = models_dict[best_model_key]\n",
    "y_pred_best = y_preds_dict[best_model_key]\n",
    "metrics_best = metrics_dict[best_model_key]\n",
    "\n",
    "# Save best model to its results directory\n",
    "best_model_path = os.path.join(RUN_DIR, f\"best_multiclass_model_{best_model_key}.pkl\")\n",
    "joblib.dump(best_model_obj, best_model_path)\n",
    "print(f\"Best model saved: {best_model_path}\\n\")\n",
    "\n",
    "# Also save to production directory for deployment\n",
    "best_model_prod_path = os.path.join(MODELS_DIR, f\"best_multiclass_attack_classification_model.pkl\")\n",
    "joblib.dump(best_model_obj, best_model_prod_path)\n",
    "print(f\"Best model also saved to production: {best_model_prod_path}\\n\")\n",
    "\n",
    "# Generate classification report for best model\n",
    "class_report = classification_report(y_test, y_pred_best,\n",
    "                                     target_names=class_labels,\n",
    "                                     digits=4)\n",
    "\n",
    "# Create detailed report text\n",
    "report_text = f\"\"\"\n",
    "{'='*70}\n",
    "MULTICLASS ATTACK CLASSIFICATION - BEST MODEL REPORT\n",
    "{'='*70}\n",
    "\n",
    "Execution Timestamp: {timestamp}\n",
    "Best Model: {best_model_key.upper()}\n",
    "Results Location: {best_model_path}\n",
    "Production Location: {best_model_prod_path}\n",
    "Attack Classes: {', '.join(class_labels)}\n",
    "Total Classes: {len(class_labels)}\n",
    "\n",
    "{'='*70}\n",
    "MODEL PERFORMANCE METRICS\n",
    "{'='*70}\n",
    "\n",
    "Accuracy (overall):         {metrics_best['accuracy']:.4f}\n",
    "Precision (macro):          {metrics_best['precision_macro']:.4f}\n",
    "Recall (macro):             {metrics_best['recall_macro']:.4f}\n",
    "F1-Score (macro):           {metrics_best['f1_macro']:.4f}\n",
    "F1-Score (weighted):        {metrics_best['f1_weighted']:.4f}\n",
    "\n",
    "Confusion Matrix Shape: {metrics_best['confusion_matrix'].shape}\n",
    "\n",
    "{'='*70}\n",
    "CLASSIFICATION REPORT\n",
    "{'='*70}\n",
    "\n",
    "{class_report}\n",
    "\n",
    "{'='*70}\n",
    "BEST MODEL HYPERPARAMETERS\n",
    "{'='*70}\n",
    "\n",
    "{str(model_defs[best_model_key]['param_grid'])}\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "# Save report to file\n",
    "report_path = os.path.join(RUN_DIR, \"02_best_model_report.txt\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(\"Best model report generated and saved\")\n",
    "print(report_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dadaeb5",
   "metadata": {},
   "source": [
    "## 9. Model Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Performance metrics comparison across models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle(\"Multiclass Model Performance Comparison\", fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = [\"test_accuracy\", \"test_precision_macro\", \"test_recall_macro\",\n",
    "                   \"test_f1_macro\", \"test_f1_weighted\", \"cv_f1_macro\"]\n",
    "colors = ['#FF6B6B' if model == best_model_key else '#4ECDC4'\n",
    "          for model in metrics_df[\"model\"]]\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    bars = ax.bar(metrics_df[\"model\"], metrics_df[metric], color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax.set_ylabel(metric.replace(\"test_\", \"\").replace(\"_\", \" \"), fontsize=10)\n",
    "    ax.set_title(metric.replace(\"test_\", \"\").replace(\"_\", \" \").upper(), fontsize=11, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_path = os.path.join(RUN_DIR, \"03_models_comparison_metrics.png\")\n",
    "plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: models_comparison_metrics.png\")\n",
    "\n",
    "# 2. F1-score ranking\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sorted_df = metrics_df.sort_values(\"test_f1_macro\", ascending=True)\n",
    "colors_rank = ['#FF6B6B' if model == best_model_key else '#95E1D3'\n",
    "               for model in sorted_df[\"model\"]]\n",
    "bars = ax.barh(sorted_df[\"model\"], sorted_df[\"test_f1_macro\"], color=colors_rank, edgecolor='black', alpha=0.85)\n",
    "\n",
    "ax.set_xlabel(\"F1-Score (Macro)\", fontsize=12)\n",
    "ax.set_title(\"Models Ranked by F1-Score (Macro)\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([0, 1.05])\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for idx, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "            f' {width:.4f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "ranking_path = os.path.join(RUN_DIR, \"04_f1_score_ranking.png\")\n",
    "plt.savefig(ranking_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: f1_score_ranking.png\")\n",
    "\n",
    "# 3. Metrics heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "metrics_for_heatmap = metrics_df[[\n",
    "    \"model\", \"test_accuracy\", \"test_precision_macro\",\n",
    "    \"test_recall_macro\", \"test_f1_macro\", \"test_f1_weighted\"\n",
    "]].set_index(\"model\")\n",
    "sns.heatmap(metrics_for_heatmap.T, annot=True, fmt='.4f', cmap='RdYlGn',\n",
    "            cbar_kws={'label': 'Score'}, ax=ax, linewidths=0.5)\n",
    "ax.set_title(\"Multiclass Model Metrics Heatmap\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "heatmap_path = os.path.join(RUN_DIR, \"05_metrics_heatmap.png\")\n",
    "plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: metrics_heatmap.png\")\n",
    "\n",
    "print(\"\\nAll comparison visualizations generated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75840e83",
   "metadata": {},
   "source": [
    "## 10. Execution Summary and Results Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01809fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RESULTS DIRECTORY STRUCTURE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# List all generated files in results directory\n",
    "for root, dirs, files in os.walk(RUN_DIR):\n",
    "    level = root.replace(RUN_DIR, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    sub_indent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        size_str = f\"{file_size / 1024:.1f}KB\" if file_size > 1024 else f\"{file_size}B\"\n",
    "        print(f'{sub_indent}{file} ({size_str})')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Generate execution summary\n",
    "summary_text = f\"\"\"\n",
    "MULTICLASS ATTACK CLASSIFICATION RESULTS\n",
    "\n",
    "Attack Classes: {len(class_labels)}\n",
    "Attack Types: {', '.join(class_labels)}\n",
    "\n",
    "Total Models Trained: {len(MODELS_TO_RUN)}\n",
    "Models: {', '.join([m.upper() for m in MODELS_TO_RUN])}\n",
    "\n",
    "Best Performing Model: {best_model_key.upper()}\n",
    "Best F1-Score (Macro): {best_model_f1:.4f}\n",
    "\n",
    "Results Location: {os.path.abspath(RUN_DIR)}\n",
    "\n",
    "Generated Files:\n",
    "- 01_metrics_summary_all_models.csv: Comprehensive metrics for all models\n",
    "- 02_best_model_report.txt: Detailed analysis of the best model\n",
    "- 03_models_comparison_metrics.png: Performance metrics comparison\n",
    "- 04_f1_score_ranking.png: Model ranking by F1-score\n",
    "- 05_metrics_heatmap.png: Metrics heatmap visualization\n",
    "\n",
    "Model-Specific Outputs:\n",
    "\"\"\"\n",
    "\n",
    "for model_key in MODELS_TO_RUN:\n",
    "    model_dir = os.path.join(RUN_DIR, model_key)\n",
    "    if os.path.exists(model_dir):\n",
    "        files = os.listdir(model_dir)\n",
    "        summary_text += f\"\\n  {model_key.upper()}:\\n\"\n",
    "        for file in sorted(files):\n",
    "            summary_text += f\"    - {file}\\n\"\n",
    "\n",
    "summary_text += f\"\"\"\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "# Save summary to file\n",
    "summary_path = os.path.join(RUN_DIR, \"00_EXECUTION_SUMMARY.txt\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(summary_text)\n",
    "print(f\"Summary saved to: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
