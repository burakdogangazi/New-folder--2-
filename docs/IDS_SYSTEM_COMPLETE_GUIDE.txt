DUAL-MODEL IDS - COMPLETE SYSTEM ARCHITECTURE & CONFIDENCE-BASED ROUTING GUIDE
================================================================================

EXECUTIVE SUMMARY
================================================================================
This document describes a production-ready Intrusion Detection System (IDS) with:
- Two-stage classification pipeline (Binary + Multiclass)
- Confidence-based routing logic with three decision tiers
- Integration with SIEM, Firewall, and Security Operations Center (SOC)
- Real-time threat detection and response

SYSTEM OVERVIEW
================================================================================

STAGE 1: BINARY CLASSIFICATION (Attack vs Benign)
├─ Input: Network traffic features (PCA-reduced to 40-50 dimensions)
├─ Output: "Attack" or "Benign" with confidence score (0.0-1.0)
├─ Models: Logistic Regression, KNN, Naive Bayes, Decision Tree, Random Forest, SVM
└─ Best Performer: Random Forest

STAGE 2: MULTICLASS CLASSIFICATION (Attack Type Identification)
├─ Input: Attack traffic only (filtered from Stage 1)
├─ Output: Attack type (DDoS, Injection, Scanning, etc.) with confidence
├─ Models: Same 6 classifiers, multiclass-optimized
└─ Best Performer: Random Forest

STAGE 3: CONFIDENCE-BASED ROUTING (Decision & Action Assignment)
├─ HIGH Confidence (≥85%): CRITICAL - Immediate automated response
├─ MEDIUM Confidence (60-84%): WARNING - Investigation required
└─ LOW Confidence (<60%): INFO - Routine logging only


CONFIDENCE LEVELS & ROUTING DECISIONS
================================================================================

HIGH CONFIDENCE (>85%) - CONFIRMED ATTACK
──────────────────────────────────────────
Classification: Confirmed attack with specific type identification
Priority: CRITICAL
Response Time: Immediate (automated)

Actions:
  1. SIEM Alert Generation
     - Send event to SIEM system (Splunk, ELK, ArcSight)
     - Include attack type, confidence score, source/dest IP
     - Mark as priority P1

  2. Automatic Firewall Blocking
     - Execute firewall rule to block source IP
     - Drop all traffic from attacker
     - Log all blocked packets

  3. Incident Ticket Creation
     - Create P1 incident in ticketing system (Jira/ServiceNow)
     - Include all classification details
     - Assign to SOC team

  4. SOC Notification
     - Alert Security Operations Center immediately
     - Provide attack details and recommended response
     - Escalate to incident commander

  5. Forensic Analysis Initiation
     - Enable packet capture on affected flows
     - Collect full packet data for investigation
     - Preserve evidence for post-incident review

Example:
  Sample: traffic_12345
  Confidence: 95%
  Classification: DDoS Attack (Botnet-based)
  Actions: BLOCK IP 192.168.1.100 -> SIEM ALERT -> SOC NOTIFY -> TICKET P1


MEDIUM CONFIDENCE (60-85%) - SUSPICIOUS ACTIVITY
───────────────────────────────────────────────────
Classification: Suspicious activity requiring human investigation
Priority: HIGH
Response Time: Investigation queue (analyst review)

Actions:
  1. Traffic Logging with Elevated Priority
     - Log detailed flow information
     - Mark as high-priority in log system
     - Enable for easy analyst retrieval

  2. Sandbox Analysis Queued
     - Submit suspicious samples to sandbox environment
     - Detonation in isolated VM
     - Analyze behavior and indicators of compromise

  3. Temporary Rate Limiting
     - Apply firewall rate-limit rule (50% of normal)
     - Reduce potential attack impact
     - Monitor for legitimate user complaints

  4. Tier-1 Security Analyst Alert
     - Queue for analyst review
     - Provide summary and confidence metrics
     - Allow manual decision on escalation

  5. Threat Intelligence Enrichment
     - Query IP reputation databases
     - Check against known malicious IPs
     - Cross-reference with threat feeds
     - Historical attack patterns lookup

  6. Enhanced Monitoring
     - Enable increased logging for source IP
     - Monitor related network segments
     - Track follow-up suspicious activities

Example:
  Sample: traffic_67890
  Confidence: 72%
  Classification: Possible Scanning Activity
  Actions: RATE LIMIT -> LOG -> QUEUE FOR ANALYST -> SANDBOX -> THREAT INTEL


LOW CONFIDENCE (<60%) - LIKELY BENIGN
──────────────────────────────────────
Classification: Likely benign network activity
Priority: INFO
Response Time: None (background processing)

Actions:
  1. Standard Logging for Baseline Monitoring
     - Record to standard traffic logs
     - Include in daily statistics
     - No alert generation

  2. Periodic Sampling (Quality Assurance)
     - Sample 1-2% of low-confidence samples
     - Manual review by security analysts
     - Identify false negatives

  3. Model Retraining Dataset Collection
     - Collect samples for offline analysis
     - Verify labels with PCAP review
     - Include in next month's retraining dataset

  4. No Immediate Operational Impact
     - No blocking, rate limiting, or alerts
     - Normal traffic processing continues
     - Typical web, email, DNS traffic

  5. Statistical Tracking
     - Update baseline traffic statistics
     - Calculate normal traffic patterns
     - Detect anomalies via deviation analysis

Example:
  Sample: traffic_11111
  Confidence: 45%
  Classification: Benign (Normal Web Traffic)
  Actions: LOG ONLY -> STATS UPDATE -> NO ALERT


CONFIDENCE-BASED ROUTING DECISION MATRIX
================================================================================

Confidence Level | Classification        | SIEM | Firewall | SOC | Ticket
─────────────────────────────────────────────────────────────────────────────
HIGH (>85%)      | Confirmed DDoS        | YES  | BLOCK    | YES | P1
HIGH (>85%)      | Confirmed Injection   | YES  | BLOCK    | YES | P1
HIGH (>85%)      | Confirmed Scanning    | YES  | BLOCK    | YES | P1

MEDIUM (60-85%)  | Suspicious DDoS       | YES  | RATE-LIM | NO  | QUEUE
MEDIUM (60-85%)  | Suspicious Injection  | YES  | RATE-LIM | NO  | QUEUE
MEDIUM (60-85%)  | Suspicious Scanning   | YES  | RATE-LIM | NO  | QUEUE

LOW (<60%)       | Likely Benign         | NO   | NO       | NO  | NO
LOW (<60%)       | Unclassified          | NO   | NO       | NO  | NO


INTEGRATION POINTS
================================================================================

SIEM SYSTEM (Splunk, ELK, ArcSight)
├─ HIGH confidence: Immediate alert with full context
├─ MEDIUM confidence: Queued alert for analyst review
└─ LOW confidence: Background event logging only

FIREWALL SYSTEM (Palo Alto, F5, pfSense)
├─ HIGH confidence: Automatic IP blocking rule deployment
├─ MEDIUM confidence: Rate limiting and traffic shaping
└─ LOW confidence: No action, monitoring only

TICKETING SYSTEM (Jira, ServiceNow)
├─ HIGH confidence: Create P1 incident, assign to SOC
├─ MEDIUM confidence: Queue for review, no automatic creation
└─ LOW confidence: No ticket creation

SOC DASHBOARD (Grafana, Custom)
├─ Real-time metrics: Attacks per type, confidence distribution
├─ Top threat IPs: Attack frequency ranking
├─ False positive rate: Quality metrics
└─ Incident volume: Workload distribution

THREAT INTELLIGENCE (AlienVault, Shodan)
├─ IP reputation lookup
├─ Known malicious domain checking
├─ Vulnerability feed integration
└─ Historical attack pattern analysis

MODEL RETRAINING PIPELINE
├─ Collect LOW confidence samples (≥100/day)
├─ Verify with human review (malware analyst)
├─ Add to training dataset (weekly)
└─ Retrain models (monthly or on-demand)


PERFORMANCE SPECIFICATIONS
================================================================================

Latency per Sample:
├─ Feature Extraction: ~50ms
├─ Feature Scaling (StandardScaler): ~5ms
├─ PCA Transform: ~3ms
├─ Binary Classification: ~2ms
├─ Multiclass Classification: ~2ms (if Attack)
├─ Confidence Routing: <1ms
└─ Total: 60-65ms (Attack) / 55ms (Benign)

Throughput:
├─ Single stream: 15-20 packets per millisecond
├─ Parallel processing: 100+ concurrent streams
└─ Daily capacity: 1.3+ billion flows

Memory Usage:
├─ Binary model: ~25MB
├─ Multiclass model: ~25MB
├─ Feature scalers & PCA: ~30MB
├─ Cache & buffers: ~100MB
└─ Total: ~180MB

False Positive Rate (FPR):
├─ Overall: 2-3%
├─ HIGH confidence: <1%
├─ MEDIUM confidence: 5-10%
└─ FALSE NEGATIVE rate: <0.5%


DEPLOYMENT REQUIREMENTS
================================================================================

Hardware:
├─ CPU: 4-8 cores (Intel Xeon or equivalent)
├─ Memory: 8GB minimum (16GB recommended)
├─ Storage: 500GB (logs + models + cache)
└─ Network: Dedicated TAP or SPAN port

Software:
├─ Python 3.10+
├─ scikit-learn 1.0+
├─ pandas, numpy
├─ joblib (model persistence)
└─ Integration SDKs for SIEM/Firewall

Network Integration:
├─ SPAN port from core switch
├─ NetFlow/sFlow export
├─ Packet capture via libpcap
└─ API access to firewall/SIEM


OPERATIONAL WORKFLOW
================================================================================

1. DEPLOYMENT PHASE
   • Load trained models into production
   • Configure feature extraction parameters
   • Set integration endpoints (SIEM, firewall, SOC)
   • Initialize all external connections
   • Enable traffic capture

2. MONITORING PHASE
   • Real-time traffic analysis (24/7)
   • Confidence-based alert routing
   • SIEM event generation
   • Automatic firewall rule deployment
   • SOC dashboard updates

3. INCIDENT RESPONSE (HIGH Confidence)
   • SIEM sends alert to SOC immediately
   • Automatic IP blocking engaged
   • P1 incident ticket created
   • Security team notified via escalation
   • Forensic packet capture begins

4. INVESTIGATION (MEDIUM Confidence)
   • Tier-1 analyst receives alert queue
   • Enhanced monitoring for source IP
   • Sandbox detonation analysis
   • Threat intelligence enrichment
   • Manual escalation decision

5. MAINTENANCE (LOW Confidence)
   • Standard logging and statistics
   • Sample collection for QA
   • Dataset verification by analysts
   • Monthly model retraining
   • Performance metrics tracking


EXAMPLE ATTACK SCENARIOS
================================================================================

SCENARIO 1: DDoS Attack
────────────────────────
Traffic Pattern: Multiple sources -> Single destination, High volume
Stage 1 Result: ATTACK (98% confidence)
Stage 2 Result: DDoS (95% confidence)
Confidence Level: HIGH
Routing Decision: CRITICAL
Actions:
  ✓ IMMEDIATE: Block traffic at network perimeter
  ✓ IMMEDIATE: Generate SIEM alert (Red severity)
  ✓ IMMEDIATE: Create P1 incident ticket
  ✓ IMMEDIATE: Notify SOC - "Active DDoS attack underway"
  ✓ IMMEDIATE: Enable full packet capture

Response Time: <1 second from detection


SCENARIO 2: SQL Injection Attempt
──────────────────────────────────
Traffic Pattern: HTTP POST with malicious payloads to database
Stage 1 Result: ATTACK (89% confidence)
Stage 2 Result: Injection (82% confidence)
Confidence Level: MEDIUM
Routing Decision: HIGH PRIORITY
Actions:
  ✓ LOG: Detailed flow logging with elevated priority
  ✓ SANDBOX: Submit payload for detonation analysis
  ✓ RATE_LIMIT: Apply 50% rate limiting to source
  ✓ ANALYST: Queue for tier-1 security analyst review
  ✓ THREAT_INTEL: Check IP reputation database

Response Time: Analyst review within 30 minutes


SCENARIO 3: Port Scanning
──────────────────────────
Traffic Pattern: Sequential TCP SYN to multiple ports
Stage 1 Result: ATTACK (74% confidence)
Stage 2 Result: Scanning (68% confidence)
Confidence Level: MEDIUM
Routing Decision: HIGH PRIORITY
Actions:
  ✓ LOG: Enhanced logging for reconnaissance activity
  ✓ MONITOR: Watch for follow-up malicious activity
  ✓ RATE_LIMIT: Apply rate limiting
  ✓ ANALYST: Queue for security team
  ✓ BASELINE: Update threat profile

Response Time: Analyst review within 1 hour


SCENARIO 4: Legitimate Web Traffic
────────────────────────────────────
Traffic Pattern: Normal HTTP/HTTPS to known domains
Stage 1 Result: BENIGN (92% confidence)
Stage 2 Result: (skipped)
Confidence Level: LOW
Routing Decision: INFO
Actions:
  ✓ LOG: Standard traffic logging
  ✓ NO_ALERT: No alerts generated
  ✓ STATS: Update baseline statistics
  ✓ SAMPLE: Maybe include in 1% QA sample
  ✓ CONTINUE: Normal processing

Response Time: No human intervention needed


SYSTEM STATES & TRANSITIONS
================================================================================

IDLE
├─ (Start monitoring)
└─ ▼
PROCESSING
├─ Feature extraction
├─ Binary classification
├─ (If Attack) Multiclass classification
├─ Confidence scoring
└─ ▼
ROUTING
├─ Evaluate confidence level
├─ Assign priority
├─ Generate actions
└─ ▼
ACTION_EXECUTION
├─ HIGH: Execute immediately (blocking, alerts)
├─ MEDIUM: Queue for analyst
├─ LOW: Background logging
└─ ▼
LOGGING
├─ Record decision
├─ Log all actions
├─ Update statistics
└─ (Return to PROCESSING for next sample)


CONFIGURATION PARAMETERS
================================================================================

HIGH_CONFIDENCE_THRESHOLD = 0.85 (≥85%)
  └─ Controls sensitivity for automatic blocking
  └─ Higher = fewer false positives, might miss attacks
  └─ Lower = faster response, more false positives

MEDIUM_CONFIDENCE_THRESHOLD = 0.60 (60-84%)
  └─ Controls sensitivity for analyst alerts
  └─ Higher = easier analyst workload, miss suspicious activity
  └─ Lower = more alerts to investigate

RATE_LIMIT_PERCENTAGE = 0.50 (50% of normal)
  └─ Applied to MEDIUM confidence traffic
  └─ Prevents attack while investigation ongoing

SAMPLE_RATIO = 0.01 (1% sampling)
  └─ Percentage of LOW confidence to sample for QA
  └─ Ensures model quality over time

RETRAINING_INTERVAL = 30 days
  └─ Frequency of model updates
  └─ Based on collected low-confidence samples
  └─ Can be reduced if high false positive rate


TROUBLESHOOTING & TUNING
================================================================================

HIGH FALSE POSITIVE RATE (>5%):
  Solution 1: Increase HIGH_CONFIDENCE_THRESHOLD (0.85 → 0.90)
  Solution 2: Review MEDIUM alerts - adjust threshold (0.60 → 0.65)
  Solution 3: Retrain models with recent benign samples
  
TOO MANY ANALYST ALERTS:
  Solution 1: Increase MEDIUM_CONFIDENCE_THRESHOLD (0.60 → 0.70)
  Solution 2: Improve feature engineering to reduce noise
  Solution 3: Filter by attack type or source IP

MISSING ATTACKS (FALSE NEGATIVES):
  Solution 1: Decrease thresholds to catch more attacks
  Solution 2: Retrain with recent attack patterns
  Solution 3: Add new attack signatures to training data

HIGH LATENCY (>100ms per sample):
  Solution 1: Use smaller PCA dimensions (retain 90% instead of 95%)
  Solution 2: Deploy on faster hardware
  Solution 3: Implement batch processing instead of streaming


MONITORING & DASHBOARDS
================================================================================

Key Metrics to Track:
├─ Attack detection rate (% of attacks caught)
├─ False positive rate (legitimate flagged as attack)
├─ False negative rate (attacks missed)
├─ Average confidence score distribution
├─ Processing latency (ms per sample)
├─ Model accuracy (test set performance)
├─ System uptime (% of operational time)
├─ Analyst response time (minutes to review)

Recommended Dashboard Views:
├─ Real-time alerts (by confidence level)
├─ Attack types distribution (pie chart)
├─ Top threat IPs (bar chart)
├─ Confidence score distribution (histogram)
├─ System performance metrics
├─ Incident ticket status
├─ Model performance trends


SECURITY BEST PRACTICES
================================================================================

Model Security:
✓ Store models in secure, encrypted storage
✓ Version control all model updates
✓ Use signed models to prevent tampering
✓ Regular model integrity checks

Access Control:
✓ Limit IDS configuration access to SOC team
✓ Require authentication for all APIs
✓ Log all configuration changes
✓ Use role-based access control (RBAC)

Data Protection:
✓ Encrypt all traffic logs
✓ Secure storage for PCAP files
✓ Implement data retention policies
✓ GDPR compliance for sensitive data

Incident Response:
✓ Maintain incident response playbooks
✓ Regular tabletop exercises
✓ Document all major incidents
✓ Continuous improvement process


NEXT STEPS & IMPLEMENTATION
================================================================================

Week 1-2: Preparation
  □ Acquire hardware/cloud resources
  □ Deploy Python environment
  □ Load trained models
  □ Configure feature extraction

Week 3-4: Integration
  □ Connect to SIEM system
  □ Configure firewall API access
  □ Setup ticketing system integration
  □ Test all integration points

Week 5-6: Testing
  □ Inject known attacks (red team exercise)
  □ Verify alerts and blocking work
  □ Test confidence routing logic
  □ Measure latency and throughput

Week 7-8: Pilot Deployment
  □ Deploy to production (traffic mirror/TAP)
  □ Monitor for 2 weeks
  □ Collect metrics and logs
  □ Fine-tune thresholds based on results

Week 9+: Full Production
  □ Switch to active blocking
  □ Monitor 24/7 for incidents
  □ Collect low-confidence samples
  □ Plan monthly model retraining

================================================================================
END OF ARCHITECTURE DOCUMENTATION
================================================================================
